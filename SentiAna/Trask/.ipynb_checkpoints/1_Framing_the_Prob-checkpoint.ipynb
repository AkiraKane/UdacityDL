{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define path to the data\n",
    "## Ubuntu path\n",
    "#path = \"/home/isaac/UdacityDL/SentiAna/Trask/\"\n",
    "## Windows path\n",
    "path = \"C:/UdacityDL/SentiAna/Trask/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path+\"reviews.txt\", \"r+\") as file:\n",
    "    reviews = file.readlines() \n",
    "    ##use readlines as it will be separated by \\n; read will read the whole thing as one big chunk of array\n",
    "    file.close()\n",
    "\n",
    "with open(path+\"labels.txt\", \"r+\") as file:\n",
    "    labels = file.readlines()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]\n",
    "## Notice \\n at the end of each review[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]\n",
    "## Notice \\n at the end of each label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_clean = list(map(lambda x: x[:-1], reviews))\n",
    "reviews_clean[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_clean = list(map(lambda x: x[:-1], labels))\n",
    "labels_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Theory Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10780,   530,  9874,  1277,  9819, 11765,  1693, 10417, 14721, 20023])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rand = np.random.randint(0,len(reviews_clean),10)\n",
    "my_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive : michelle rodriguez is the defining actress who could be the charging force for other actresses to lo\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "positive : i disagree with anyone who done  t like this movie .  br    br   i used to love this movie when i wa\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "positive : there has never been anything like it  that  s for sure . this episodic  seemingly redundant trilogy\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : honestly  the concept behind  masters of horror  had something going for it . big  time horror direc\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : this movie is one of the worst ones of the year . the main characters have no chemistry and the acti\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : from a plot and movement standpoint  this movie was terrible . i found myself looking at the clock i\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : this was an impulse pick up for me from the local video store . don  t make the same mistake i did .\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : haunted boat sells itself as  the fog  meets  open water  . in many ways this is accurate . there ar\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : having just watched this movie  i almost feel like having wasted  hours of my life  but i guess ther\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "negative : after a series of power  outages on a remote island zoo  genetically engineered sabertooth tigers ar\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for num in my_rand:\n",
    "    print(labels_clean[num]+\" : \"+reviews_clean[num][:100]+\"\\n\")\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialization\n",
    "positive_count = Counter()\n",
    "negative_count = Counter()\n",
    "total_count = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    if(labels_clean[i] == \"positive\"):\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            positive_count[word] += 1\n",
    "            total_count[word] += 1\n",
    "    else:\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            negative_count[word] += 1\n",
    "            total_count[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 537968),\n",
       " ('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822),\n",
       " ('movie', 19074),\n",
       " ('his', 17227),\n",
       " ('on', 17008),\n",
       " ('you', 16681),\n",
       " ('he', 16282),\n",
       " ('are', 14807),\n",
       " ('not', 14272),\n",
       " ('t', 13720),\n",
       " ('one', 13655),\n",
       " ('have', 12587),\n",
       " ('\\n', 12500),\n",
       " ('be', 12416),\n",
       " ('by', 11997),\n",
       " ('all', 11942),\n",
       " ('who', 11464),\n",
       " ('an', 11294),\n",
       " ('at', 11234),\n",
       " ('from', 10767),\n",
       " ('her', 10474),\n",
       " ('they', 9895),\n",
       " ('has', 9186),\n",
       " ('so', 9154),\n",
       " ('like', 9038),\n",
       " ('about', 8313),\n",
       " ('very', 8305),\n",
       " ('out', 8134),\n",
       " ('there', 8057),\n",
       " ('she', 7779),\n",
       " ('what', 7737),\n",
       " ('or', 7732),\n",
       " ('good', 7720),\n",
       " ('more', 7521),\n",
       " ('when', 7456),\n",
       " ('some', 7441),\n",
       " ('if', 7285),\n",
       " ('just', 7152),\n",
       " ('can', 7001),\n",
       " ('story', 6780),\n",
       " ('time', 6515),\n",
       " ('my', 6488),\n",
       " ('great', 6419),\n",
       " ('well', 6405),\n",
       " ('up', 6321),\n",
       " ('which', 6267),\n",
       " ('their', 6107),\n",
       " ('see', 6026),\n",
       " ('also', 5550),\n",
       " ('we', 5531),\n",
       " ('really', 5476),\n",
       " ('would', 5400),\n",
       " ('will', 5218),\n",
       " ('me', 5167),\n",
       " ('had', 5148),\n",
       " ('only', 5137),\n",
       " ('him', 5018),\n",
       " ('even', 4964),\n",
       " ('most', 4864),\n",
       " ('other', 4858),\n",
       " ('were', 4782),\n",
       " ('first', 4755),\n",
       " ('than', 4736),\n",
       " ('much', 4685),\n",
       " ('its', 4622),\n",
       " ('no', 4574),\n",
       " ('into', 4544),\n",
       " ('people', 4479),\n",
       " ('best', 4319),\n",
       " ('love', 4301),\n",
       " ('get', 4272),\n",
       " ('how', 4213),\n",
       " ('life', 4199),\n",
       " ('been', 4189),\n",
       " ('because', 4079),\n",
       " ('way', 4036),\n",
       " ('do', 3941),\n",
       " ('made', 3823),\n",
       " ('films', 3813),\n",
       " ('them', 3805),\n",
       " ('after', 3800),\n",
       " ('many', 3766),\n",
       " ('two', 3733),\n",
       " ('too', 3659),\n",
       " ('think', 3655),\n",
       " ('movies', 3586),\n",
       " ('characters', 3560),\n",
       " ('character', 3514),\n",
       " ('don', 3468),\n",
       " ('man', 3460),\n",
       " ('show', 3432),\n",
       " ('watch', 3424),\n",
       " ('seen', 3414),\n",
       " ('then', 3358),\n",
       " ('little', 3341),\n",
       " ('still', 3340),\n",
       " ('make', 3303),\n",
       " ('could', 3237),\n",
       " ('never', 3226),\n",
       " ('being', 3217),\n",
       " ('where', 3173),\n",
       " ('does', 3069),\n",
       " ('over', 3017),\n",
       " ('any', 3002),\n",
       " ('while', 2899),\n",
       " ('know', 2833),\n",
       " ('did', 2790),\n",
       " ('years', 2758),\n",
       " ('here', 2740),\n",
       " ('ever', 2734),\n",
       " ('end', 2696),\n",
       " ('these', 2694),\n",
       " ('such', 2590),\n",
       " ('real', 2568),\n",
       " ('scene', 2567),\n",
       " ('back', 2547),\n",
       " ('those', 2485),\n",
       " ('though', 2475),\n",
       " ('off', 2463),\n",
       " ('new', 2458),\n",
       " ('your', 2453),\n",
       " ('go', 2440),\n",
       " ('acting', 2437),\n",
       " ('plot', 2432),\n",
       " ('world', 2429),\n",
       " ('scenes', 2427),\n",
       " ('say', 2414),\n",
       " ('through', 2409),\n",
       " ('makes', 2390),\n",
       " ('better', 2381),\n",
       " ('now', 2368),\n",
       " ('work', 2346),\n",
       " ('young', 2343),\n",
       " ('old', 2311),\n",
       " ('ve', 2307),\n",
       " ('find', 2272),\n",
       " ('both', 2248),\n",
       " ('before', 2177),\n",
       " ('us', 2162),\n",
       " ('again', 2158),\n",
       " ('series', 2153),\n",
       " ('quite', 2143),\n",
       " ('something', 2135),\n",
       " ('cast', 2133),\n",
       " ('should', 2121),\n",
       " ('part', 2098),\n",
       " ('always', 2088),\n",
       " ('lot', 2087),\n",
       " ('another', 2075),\n",
       " ('actors', 2047),\n",
       " ('director', 2040),\n",
       " ('family', 2032),\n",
       " ('between', 2016),\n",
       " ('own', 2016),\n",
       " ('m', 1998),\n",
       " ('may', 1997),\n",
       " ('same', 1972),\n",
       " ('role', 1967),\n",
       " ('watching', 1966),\n",
       " ('every', 1954),\n",
       " ('funny', 1953),\n",
       " ('doesn', 1935),\n",
       " ('performance', 1928),\n",
       " ('few', 1918),\n",
       " ('bad', 1907),\n",
       " ('look', 1900),\n",
       " ('re', 1884),\n",
       " ('why', 1855),\n",
       " ('things', 1849),\n",
       " ('times', 1832),\n",
       " ('big', 1815),\n",
       " ('however', 1795),\n",
       " ('actually', 1790),\n",
       " ('action', 1789),\n",
       " ('going', 1783),\n",
       " ('bit', 1757),\n",
       " ('comedy', 1742),\n",
       " ('down', 1740),\n",
       " ('music', 1738),\n",
       " ('must', 1728),\n",
       " ('take', 1709),\n",
       " ('saw', 1692),\n",
       " ('long', 1690),\n",
       " ('right', 1688),\n",
       " ('fun', 1686),\n",
       " ('fact', 1684),\n",
       " ('excellent', 1683),\n",
       " ('around', 1674),\n",
       " ('didn', 1672),\n",
       " ('without', 1671),\n",
       " ('thing', 1662),\n",
       " ('thought', 1639),\n",
       " ('got', 1635),\n",
       " ('each', 1630),\n",
       " ('day', 1614),\n",
       " ('feel', 1597),\n",
       " ('seems', 1596),\n",
       " ('come', 1594),\n",
       " ('done', 1586),\n",
       " ('beautiful', 1580),\n",
       " ('especially', 1572),\n",
       " ('played', 1571),\n",
       " ('almost', 1566),\n",
       " ('want', 1562),\n",
       " ('yet', 1556),\n",
       " ('give', 1553),\n",
       " ('pretty', 1549),\n",
       " ('last', 1543),\n",
       " ('since', 1519),\n",
       " ('different', 1504),\n",
       " ('although', 1501),\n",
       " ('gets', 1490),\n",
       " ('true', 1487),\n",
       " ('interesting', 1481),\n",
       " ('job', 1470),\n",
       " ('enough', 1455),\n",
       " ('our', 1454),\n",
       " ('shows', 1447),\n",
       " ('horror', 1441),\n",
       " ('woman', 1439),\n",
       " ('tv', 1400),\n",
       " ('probably', 1398),\n",
       " ('father', 1395),\n",
       " ('original', 1393),\n",
       " ('girl', 1390),\n",
       " ('point', 1379),\n",
       " ('plays', 1378),\n",
       " ('wonderful', 1372),\n",
       " ('far', 1358),\n",
       " ('course', 1358),\n",
       " ('john', 1350),\n",
       " ('rather', 1340),\n",
       " ('isn', 1328),\n",
       " ('ll', 1326),\n",
       " ('later', 1324),\n",
       " ('dvd', 1324),\n",
       " ('whole', 1310),\n",
       " ('war', 1310),\n",
       " ('d', 1307),\n",
       " ('found', 1306),\n",
       " ('away', 1306),\n",
       " ('screen', 1305),\n",
       " ('nothing', 1300),\n",
       " ('year', 1297),\n",
       " ('once', 1296),\n",
       " ('hard', 1294),\n",
       " ('together', 1280),\n",
       " ('set', 1277),\n",
       " ('am', 1277),\n",
       " ('having', 1266),\n",
       " ('making', 1265),\n",
       " ('place', 1263),\n",
       " ('might', 1260),\n",
       " ('comes', 1260),\n",
       " ('sure', 1253),\n",
       " ('american', 1248),\n",
       " ('play', 1245),\n",
       " ('kind', 1244),\n",
       " ('perfect', 1242),\n",
       " ('takes', 1242),\n",
       " ('performances', 1237),\n",
       " ('himself', 1230),\n",
       " ('worth', 1221),\n",
       " ('everyone', 1221),\n",
       " ('anyone', 1214),\n",
       " ('actor', 1203),\n",
       " ('three', 1201),\n",
       " ('wife', 1196),\n",
       " ('classic', 1192),\n",
       " ('goes', 1186),\n",
       " ('ending', 1178),\n",
       " ('version', 1168),\n",
       " ('star', 1149),\n",
       " ('enjoy', 1146),\n",
       " ('book', 1142),\n",
       " ('nice', 1132),\n",
       " ('everything', 1128),\n",
       " ('during', 1124),\n",
       " ('put', 1118),\n",
       " ('seeing', 1111),\n",
       " ('least', 1102),\n",
       " ('house', 1100),\n",
       " ('high', 1095),\n",
       " ('watched', 1094),\n",
       " ('loved', 1087),\n",
       " ('men', 1087),\n",
       " ('night', 1082),\n",
       " ('anything', 1075),\n",
       " ('believe', 1071),\n",
       " ('guy', 1071),\n",
       " ('top', 1063),\n",
       " ('amazing', 1058),\n",
       " ('hollywood', 1056),\n",
       " ('looking', 1053),\n",
       " ('main', 1044),\n",
       " ('definitely', 1043),\n",
       " ('gives', 1031),\n",
       " ('home', 1029),\n",
       " ('seem', 1028),\n",
       " ('episode', 1023),\n",
       " ('audience', 1020),\n",
       " ('sense', 1020),\n",
       " ('truly', 1017),\n",
       " ('special', 1011),\n",
       " ('second', 1009),\n",
       " ('short', 1009),\n",
       " ('fan', 1009),\n",
       " ('mind', 1005),\n",
       " ('human', 1001),\n",
       " ('recommend', 999),\n",
       " ('full', 996),\n",
       " ('black', 995),\n",
       " ('help', 991),\n",
       " ('along', 989),\n",
       " ('trying', 987),\n",
       " ('small', 986),\n",
       " ('death', 985),\n",
       " ('friends', 981),\n",
       " ('remember', 974),\n",
       " ('often', 970),\n",
       " ('said', 966),\n",
       " ('favorite', 962),\n",
       " ('heart', 959),\n",
       " ('early', 957),\n",
       " ('left', 956),\n",
       " ('until', 955),\n",
       " ('script', 954),\n",
       " ('let', 954),\n",
       " ('maybe', 937),\n",
       " ('today', 936),\n",
       " ('live', 934),\n",
       " ('less', 934),\n",
       " ('moments', 933),\n",
       " ('others', 929),\n",
       " ('brilliant', 926),\n",
       " ('shot', 925),\n",
       " ('liked', 923),\n",
       " ('become', 916),\n",
       " ('won', 915),\n",
       " ('used', 910),\n",
       " ('style', 907),\n",
       " ('mother', 895),\n",
       " ('lives', 894),\n",
       " ('came', 893),\n",
       " ('stars', 890),\n",
       " ('cinema', 889),\n",
       " ('looks', 885),\n",
       " ('perhaps', 884),\n",
       " ('read', 882),\n",
       " ('enjoyed', 879),\n",
       " ('boy', 875),\n",
       " ('drama', 873),\n",
       " ('highly', 871),\n",
       " ('given', 870),\n",
       " ('playing', 867),\n",
       " ('use', 864),\n",
       " ('next', 859),\n",
       " ('women', 858),\n",
       " ('fine', 857),\n",
       " ('effects', 856),\n",
       " ('kids', 854),\n",
       " ('entertaining', 853),\n",
       " ('need', 852),\n",
       " ('line', 850),\n",
       " ('works', 848),\n",
       " ('someone', 847),\n",
       " ('mr', 836),\n",
       " ('simply', 835),\n",
       " ('picture', 833),\n",
       " ('children', 833),\n",
       " ('face', 831),\n",
       " ('keep', 831),\n",
       " ('friend', 831),\n",
       " ('dark', 830),\n",
       " ('overall', 828),\n",
       " ('certainly', 828),\n",
       " ('minutes', 827),\n",
       " ('wasn', 824),\n",
       " ('history', 822),\n",
       " ('finally', 820),\n",
       " ('couple', 816),\n",
       " ('against', 815),\n",
       " ('son', 809),\n",
       " ('understand', 808),\n",
       " ('lost', 807),\n",
       " ('michael', 805),\n",
       " ('else', 801),\n",
       " ('throughout', 798),\n",
       " ('fans', 797),\n",
       " ('city', 792),\n",
       " ('reason', 789),\n",
       " ('written', 787),\n",
       " ('production', 787),\n",
       " ('several', 784),\n",
       " ('school', 783),\n",
       " ('based', 781),\n",
       " ('rest', 781),\n",
       " ('try', 780),\n",
       " ('dead', 776),\n",
       " ('hope', 775),\n",
       " ('strong', 768),\n",
       " ('white', 765),\n",
       " ('tell', 759),\n",
       " ('itself', 758),\n",
       " ('half', 753),\n",
       " ('person', 749),\n",
       " ('sometimes', 746),\n",
       " ('past', 744),\n",
       " ('start', 744),\n",
       " ('genre', 743),\n",
       " ('beginning', 739),\n",
       " ('final', 739),\n",
       " ('town', 738),\n",
       " ('art', 734),\n",
       " ('humor', 732),\n",
       " ('game', 732),\n",
       " ('yes', 731),\n",
       " ('idea', 731),\n",
       " ('late', 730),\n",
       " ('becomes', 729),\n",
       " ('despite', 729),\n",
       " ('able', 726),\n",
       " ('case', 726),\n",
       " ('money', 723),\n",
       " ('child', 721),\n",
       " ('completely', 721),\n",
       " ('side', 719),\n",
       " ('camera', 716),\n",
       " ('getting', 714),\n",
       " ('instead', 712),\n",
       " ('soon', 702),\n",
       " ('under', 700),\n",
       " ('viewer', 699),\n",
       " ('age', 697),\n",
       " ('days', 696),\n",
       " ('stories', 696),\n",
       " ('felt', 694),\n",
       " ('simple', 694),\n",
       " ('roles', 693),\n",
       " ('video', 688),\n",
       " ('name', 683),\n",
       " ('either', 683),\n",
       " ('doing', 677),\n",
       " ('turns', 674),\n",
       " ('wants', 671),\n",
       " ('close', 671),\n",
       " ('title', 669),\n",
       " ('wrong', 668),\n",
       " ('went', 666),\n",
       " ('james', 665),\n",
       " ('evil', 659),\n",
       " ('budget', 657),\n",
       " ('episodes', 657),\n",
       " ('relationship', 655),\n",
       " ('fantastic', 653),\n",
       " ('piece', 653),\n",
       " ('david', 651),\n",
       " ('turn', 648),\n",
       " ('murder', 646),\n",
       " ('parts', 645),\n",
       " ('brother', 644),\n",
       " ('absolutely', 643),\n",
       " ('head', 643),\n",
       " ('experience', 642),\n",
       " ('eyes', 641),\n",
       " ('sex', 638),\n",
       " ('direction', 637),\n",
       " ('called', 637),\n",
       " ('directed', 636),\n",
       " ('lines', 634),\n",
       " ('behind', 633),\n",
       " ('sort', 632),\n",
       " ('actress', 631),\n",
       " ('lead', 630),\n",
       " ('oscar', 628),\n",
       " ('including', 627),\n",
       " ('example', 627),\n",
       " ('known', 625),\n",
       " ('musical', 625),\n",
       " ('chance', 621),\n",
       " ('score', 620),\n",
       " ('already', 619),\n",
       " ('feeling', 619),\n",
       " ('hit', 619),\n",
       " ('voice', 615),\n",
       " ('moment', 612),\n",
       " ('living', 612),\n",
       " ('low', 610),\n",
       " ('supporting', 610),\n",
       " ('ago', 609),\n",
       " ('themselves', 608),\n",
       " ('reality', 605),\n",
       " ('hilarious', 605),\n",
       " ('jack', 604),\n",
       " ('told', 603),\n",
       " ('hand', 601),\n",
       " ('quality', 600),\n",
       " ('moving', 600),\n",
       " ('dialogue', 600),\n",
       " ('song', 599),\n",
       " ('happy', 599),\n",
       " ('matter', 598),\n",
       " ('paul', 598),\n",
       " ('light', 594),\n",
       " ('future', 593),\n",
       " ('entire', 592),\n",
       " ('finds', 591),\n",
       " ('gave', 589),\n",
       " ('laugh', 587),\n",
       " ('released', 586),\n",
       " ('expect', 584),\n",
       " ('fight', 581),\n",
       " ('particularly', 580),\n",
       " ('cinematography', 579),\n",
       " ('police', 579),\n",
       " ('whose', 578),\n",
       " ('type', 578),\n",
       " ('sound', 578),\n",
       " ('view', 573),\n",
       " ('enjoyable', 573),\n",
       " ('number', 572),\n",
       " ('romantic', 572),\n",
       " ('husband', 572),\n",
       " ('daughter', 572),\n",
       " ('documentary', 571),\n",
       " ('self', 570),\n",
       " ('superb', 569),\n",
       " ('modern', 569),\n",
       " ('took', 569),\n",
       " ('robert', 569),\n",
       " ('mean', 566),\n",
       " ('shown', 563),\n",
       " ('coming', 561),\n",
       " ('important', 560),\n",
       " ('king', 559),\n",
       " ('leave', 559),\n",
       " ('change', 558),\n",
       " ('somewhat', 555),\n",
       " ('wanted', 555),\n",
       " ('tells', 554),\n",
       " ('events', 552),\n",
       " ('run', 552),\n",
       " ('career', 552),\n",
       " ('country', 552),\n",
       " ('heard', 550),\n",
       " ('season', 550),\n",
       " ('greatest', 549),\n",
       " ('girls', 549),\n",
       " ('etc', 547),\n",
       " ('care', 546),\n",
       " ('starts', 545),\n",
       " ('english', 542),\n",
       " ('killer', 541),\n",
       " ('tale', 540),\n",
       " ('guys', 540),\n",
       " ('totally', 540),\n",
       " ('animation', 540),\n",
       " ('usual', 539),\n",
       " ('miss', 535),\n",
       " ('opinion', 535),\n",
       " ('easy', 531),\n",
       " ('violence', 531),\n",
       " ('songs', 530),\n",
       " ('british', 528),\n",
       " ('says', 526),\n",
       " ('realistic', 525),\n",
       " ('writing', 524),\n",
       " ('writer', 522),\n",
       " ('act', 522),\n",
       " ('comic', 521),\n",
       " ('thriller', 519),\n",
       " ('television', 517),\n",
       " ('power', 516),\n",
       " ('ones', 515),\n",
       " ('kid', 514),\n",
       " ('york', 513),\n",
       " ('novel', 513),\n",
       " ('alone', 512),\n",
       " ('problem', 512),\n",
       " ('attention', 509),\n",
       " ('involved', 508),\n",
       " ('kill', 507),\n",
       " ('extremely', 507),\n",
       " ('seemed', 506),\n",
       " ('hero', 505),\n",
       " ('french', 505),\n",
       " ('rock', 504),\n",
       " ('stuff', 501),\n",
       " ('wish', 499),\n",
       " ('begins', 498),\n",
       " ('taken', 497),\n",
       " ('sad', 497),\n",
       " ('ways', 496),\n",
       " ('richard', 495),\n",
       " ('knows', 494),\n",
       " ('atmosphere', 493),\n",
       " ('similar', 491),\n",
       " ('surprised', 491),\n",
       " ('taking', 491),\n",
       " ('car', 491),\n",
       " ('george', 490),\n",
       " ('perfectly', 490),\n",
       " ('across', 489),\n",
       " ('team', 489),\n",
       " ('eye', 489),\n",
       " ('sequence', 489),\n",
       " ('room', 488),\n",
       " ('due', 488),\n",
       " ('among', 488),\n",
       " ('serious', 488),\n",
       " ('powerful', 488),\n",
       " ('strange', 487),\n",
       " ('order', 487),\n",
       " ('cannot', 487),\n",
       " ('b', 487),\n",
       " ('beauty', 486),\n",
       " ('famous', 485),\n",
       " ('happened', 484),\n",
       " ('tries', 484),\n",
       " ('herself', 484),\n",
       " ('myself', 484),\n",
       " ('class', 483),\n",
       " ('four', 482),\n",
       " ('cool', 481),\n",
       " ('release', 479),\n",
       " ('anyway', 479),\n",
       " ('theme', 479),\n",
       " ('opening', 478),\n",
       " ('entertainment', 477),\n",
       " ('slow', 475),\n",
       " ('ends', 475),\n",
       " ('unique', 475),\n",
       " ('exactly', 475),\n",
       " ('easily', 474),\n",
       " ('level', 474),\n",
       " ('o', 474),\n",
       " ('red', 474),\n",
       " ('interest', 472),\n",
       " ('happen', 471),\n",
       " ('crime', 470),\n",
       " ('viewing', 468),\n",
       " ('sets', 467),\n",
       " ('memorable', 467),\n",
       " ('stop', 466),\n",
       " ('group', 466),\n",
       " ('problems', 463),\n",
       " ('dance', 463),\n",
       " ('working', 463),\n",
       " ('sister', 463),\n",
       " ('message', 463),\n",
       " ('knew', 462),\n",
       " ('mystery', 461),\n",
       " ('nature', 461),\n",
       " ('bring', 460),\n",
       " ('believable', 459),\n",
       " ('thinking', 459),\n",
       " ('brought', 459),\n",
       " ('mostly', 458),\n",
       " ('disney', 457),\n",
       " ('couldn', 457),\n",
       " ('society', 456),\n",
       " ('lady', 455),\n",
       " ('within', 455),\n",
       " ('blood', 454),\n",
       " ('parents', 453),\n",
       " ('upon', 453),\n",
       " ('viewers', 453),\n",
       " ('meets', 452),\n",
       " ('form', 452),\n",
       " ('peter', 452),\n",
       " ('tom', 452),\n",
       " ('usually', 452),\n",
       " ('soundtrack', 452),\n",
       " ('local', 450),\n",
       " ('certain', 448),\n",
       " ('follow', 448),\n",
       " ('whether', 447),\n",
       " ('possible', 446),\n",
       " ('emotional', 445),\n",
       " ('killed', 444),\n",
       " ('above', 444),\n",
       " ('de', 444),\n",
       " ('god', 443),\n",
       " ('middle', 443),\n",
       " ('needs', 442),\n",
       " ('happens', 442),\n",
       " ('flick', 442),\n",
       " ('masterpiece', 441),\n",
       " ('period', 440),\n",
       " ('major', 440),\n",
       " ('named', 439),\n",
       " ('haven', 439),\n",
       " ('particular', 438),\n",
       " ('th', 438),\n",
       " ('earth', 437),\n",
       " ('feature', 437),\n",
       " ('stand', 436),\n",
       " ('words', 435),\n",
       " ('typical', 435),\n",
       " ('elements', 433),\n",
       " ('obviously', 433),\n",
       " ('romance', 431),\n",
       " ('jane', 430),\n",
       " ('yourself', 427),\n",
       " ('showing', 427),\n",
       " ('brings', 426),\n",
       " ('fantasy', 426),\n",
       " ('guess', 423),\n",
       " ('america', 423),\n",
       " ('unfortunately', 422),\n",
       " ('huge', 422),\n",
       " ('indeed', 421),\n",
       " ('running', 421),\n",
       " ('talent', 420),\n",
       " ('stage', 419),\n",
       " ('started', 418),\n",
       " ('leads', 417),\n",
       " ('sweet', 417),\n",
       " ('japanese', 417),\n",
       " ('poor', 416),\n",
       " ('deal', 416),\n",
       " ('incredible', 413),\n",
       " ('personal', 413),\n",
       " ('fast', 412),\n",
       " ('became', 410),\n",
       " ('deep', 410),\n",
       " ('hours', 409),\n",
       " ('giving', 408),\n",
       " ('nearly', 408),\n",
       " ('dream', 408),\n",
       " ('clearly', 407),\n",
       " ('turned', 407),\n",
       " ('obvious', 406),\n",
       " ('near', 406),\n",
       " ('cut', 405),\n",
       " ('surprise', 405),\n",
       " ('era', 404),\n",
       " ('body', 404),\n",
       " ('hour', 403),\n",
       " ('female', 403),\n",
       " ('five', 403),\n",
       " ('note', 399),\n",
       " ('learn', 398),\n",
       " ('truth', 398),\n",
       " ('except', 397),\n",
       " ('feels', 397),\n",
       " ('match', 397),\n",
       " ('tony', 397),\n",
       " ('filmed', 394),\n",
       " ('clear', 394),\n",
       " ('complete', 394),\n",
       " ('street', 393),\n",
       " ('eventually', 393),\n",
       " ('keeps', 393),\n",
       " ('older', 393),\n",
       " ('lots', 393),\n",
       " ('buy', 392),\n",
       " ('william', 391),\n",
       " ('stewart', 391),\n",
       " ('fall', 390),\n",
       " ('joe', 390),\n",
       " ('meet', 390),\n",
       " ('unlike', 389),\n",
       " ('talking', 389),\n",
       " ('shots', 389),\n",
       " ('rating', 389),\n",
       " ('difficult', 389),\n",
       " ('dramatic', 388),\n",
       " ('means', 388),\n",
       " ('situation', 386),\n",
       " ('wonder', 386),\n",
       " ('present', 386),\n",
       " ('appears', 386),\n",
       " ('subject', 386),\n",
       " ('comments', 385),\n",
       " ('general', 383),\n",
       " ('sequences', 383),\n",
       " ('lee', 383),\n",
       " ('points', 382),\n",
       " ('earlier', 382),\n",
       " ('gone', 379),\n",
       " ('check', 379),\n",
       " ('suspense', 378),\n",
       " ('recommended', 378),\n",
       " ('ten', 378),\n",
       " ('third', 377),\n",
       " ('business', 377),\n",
       " ('talk', 375),\n",
       " ('leaves', 375),\n",
       " ('beyond', 375),\n",
       " ('portrayal', 374),\n",
       " ('beautifully', 373),\n",
       " ('single', 372),\n",
       " ('bill', 372),\n",
       " ('plenty', 371),\n",
       " ('word', 371),\n",
       " ('whom', 370),\n",
       " ('falls', 370),\n",
       " ('scary', 369),\n",
       " ('non', 369),\n",
       " ('figure', 369),\n",
       " ('battle', 369),\n",
       " ('using', 368),\n",
       " ('return', 368),\n",
       " ('doubt', 367),\n",
       " ('add', 367),\n",
       " ('hear', 366),\n",
       " ('solid', 366),\n",
       " ('success', 366),\n",
       " ('jokes', 365),\n",
       " ('oh', 365),\n",
       " ('touching', 365),\n",
       " ('political', 365),\n",
       " ('hell', 364),\n",
       " ('awesome', 364),\n",
       " ('boys', 364),\n",
       " ('sexual', 362),\n",
       " ('recently', 362),\n",
       " ('dog', 362),\n",
       " ('please', 361),\n",
       " ('wouldn', 361),\n",
       " ('straight', 361),\n",
       " ('features', 361),\n",
       " ('forget', 360),\n",
       " ('setting', 360),\n",
       " ('lack', 360),\n",
       " ('married', 359),\n",
       " ('mark', 359),\n",
       " ('social', 357),\n",
       " ('interested', 356),\n",
       " ('adventure', 356),\n",
       " ('actual', 355),\n",
       " ('terrific', 355),\n",
       " ('sees', 355),\n",
       " ('brothers', 355),\n",
       " ('move', 354),\n",
       " ('call', 354),\n",
       " ('various', 353),\n",
       " ('theater', 353),\n",
       " ('dr', 353),\n",
       " ('animated', 352),\n",
       " ('western', 351),\n",
       " ('baby', 350),\n",
       " ('space', 350),\n",
       " ('leading', 348),\n",
       " ('disappointed', 348),\n",
       " ('portrayed', 346),\n",
       " ('aren', 346),\n",
       " ('screenplay', 345),\n",
       " ('smith', 345),\n",
       " ('towards', 344),\n",
       " ('hate', 344),\n",
       " ('noir', 343),\n",
       " ('outstanding', 342),\n",
       " ('decent', 342),\n",
       " ('kelly', 342),\n",
       " ('directors', 341),\n",
       " ('journey', 341),\n",
       " ('none', 340),\n",
       " ('looked', 340),\n",
       " ('effective', 340),\n",
       " ('storyline', 339),\n",
       " ('caught', 339),\n",
       " ('sci', 339),\n",
       " ('fi', 339),\n",
       " ('cold', 339),\n",
       " ('mary', 339),\n",
       " ('rich', 338),\n",
       " ('charming', 338),\n",
       " ('popular', 337),\n",
       " ('rare', 337),\n",
       " ('manages', 337),\n",
       " ('harry', 337),\n",
       " ('spirit', 336),\n",
       " ('appreciate', 335),\n",
       " ('open', 335),\n",
       " ('moves', 334),\n",
       " ('basically', 334),\n",
       " ('acted', 334),\n",
       " ('inside', 333),\n",
       " ('boring', 333),\n",
       " ('century', 333),\n",
       " ('mention', 333),\n",
       " ('deserves', 333),\n",
       " ('subtle', 333),\n",
       " ('pace', 333),\n",
       " ('familiar', 332),\n",
       " ('background', 332),\n",
       " ('ben', 331),\n",
       " ('creepy', 330),\n",
       " ('supposed', 330),\n",
       " ('secret', 329),\n",
       " ('die', 328),\n",
       " ('jim', 328),\n",
       " ('question', 327),\n",
       " ('effect', 327),\n",
       " ('natural', 327),\n",
       " ('impressive', 326),\n",
       " ('rate', 326),\n",
       " ('language', 326),\n",
       " ('saying', 325),\n",
       " ('intelligent', 325),\n",
       " ('telling', 324),\n",
       " ('realize', 324),\n",
       " ('material', 324),\n",
       " ('scott', 324),\n",
       " ('singing', 323),\n",
       " ('dancing', 322),\n",
       " ('visual', 321),\n",
       " ('adult', 321),\n",
       " ('imagine', 321),\n",
       " ('kept', 320),\n",
       " ('office', 320),\n",
       " ('uses', 319),\n",
       " ('pure', 318),\n",
       " ('wait', 318),\n",
       " ('stunning', 318),\n",
       " ('review', 317),\n",
       " ('previous', 317),\n",
       " ('copy', 317),\n",
       " ('seriously', 317),\n",
       " ('reading', 316),\n",
       " ('create', 316),\n",
       " ('hot', 316),\n",
       " ('created', 316),\n",
       " ('magic', 316),\n",
       " ('somehow', 316),\n",
       " ('stay', 315),\n",
       " ('attempt', 315),\n",
       " ('escape', 315),\n",
       " ('crazy', 315),\n",
       " ('air', 315),\n",
       " ('frank', 315),\n",
       " ('hands', 314),\n",
       " ('filled', 313),\n",
       " ('expected', 312),\n",
       " ('average', 312),\n",
       " ('surprisingly', 312),\n",
       " ('complex', 311),\n",
       " ('quickly', 310),\n",
       " ('successful', 310),\n",
       " ('studio', 310),\n",
       " ('plus', 309),\n",
       " ('male', 309),\n",
       " ('co', 307),\n",
       " ('images', 306),\n",
       " ('casting', 306),\n",
       " ('following', 306),\n",
       " ('minute', 306),\n",
       " ('exciting', 306),\n",
       " ('members', 305),\n",
       " ('follows', 305),\n",
       " ('themes', 305),\n",
       " ('german', 305),\n",
       " ('reasons', 305),\n",
       " ('e', 305),\n",
       " ('touch', 304),\n",
       " ('edge', 304),\n",
       " ('free', 304),\n",
       " ('cute', 304),\n",
       " ('genius', 304),\n",
       " ('outside', 303),\n",
       " ('reviews', 302),\n",
       " ('admit', 302),\n",
       " ('ok', 302),\n",
       " ('younger', 302),\n",
       " ('fighting', 301),\n",
       " ('odd', 301),\n",
       " ('master', 301),\n",
       " ('recent', 300),\n",
       " ('thanks', 300),\n",
       " ('break', 300),\n",
       " ('comment', 300),\n",
       " ('apart', 299),\n",
       " ('emotions', 298),\n",
       " ('lovely', 298),\n",
       " ('begin', 298),\n",
       " ('doctor', 297),\n",
       " ('party', 297),\n",
       " ('italian', 297),\n",
       " ('la', 296),\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 548962),\n",
       " ('.', 167538),\n",
       " ('the', 163389),\n",
       " ('a', 79321),\n",
       " ('and', 74385),\n",
       " ('of', 69009),\n",
       " ('to', 68974),\n",
       " ('br', 52637),\n",
       " ('is', 50083),\n",
       " ('it', 48327),\n",
       " ('i', 46880),\n",
       " ('in', 43753),\n",
       " ('this', 40920),\n",
       " ('that', 37615),\n",
       " ('s', 31546),\n",
       " ('was', 26291),\n",
       " ('movie', 24965),\n",
       " ('for', 21927),\n",
       " ('but', 21781),\n",
       " ('with', 20878),\n",
       " ('as', 20625),\n",
       " ('t', 20361),\n",
       " ('film', 19218),\n",
       " ('you', 17549),\n",
       " ('on', 17192),\n",
       " ('not', 16354),\n",
       " ('have', 15144),\n",
       " ('are', 14623),\n",
       " ('be', 14541),\n",
       " ('he', 13856),\n",
       " ('one', 13134),\n",
       " ('they', 13011),\n",
       " ('\\n', 12500),\n",
       " ('at', 12279),\n",
       " ('his', 12147),\n",
       " ('all', 12036),\n",
       " ('so', 11463),\n",
       " ('like', 11238),\n",
       " ('there', 10775),\n",
       " ('just', 10619),\n",
       " ('by', 10549),\n",
       " ('or', 10272),\n",
       " ('an', 10266),\n",
       " ('who', 9969),\n",
       " ('from', 9731),\n",
       " ('if', 9518),\n",
       " ('about', 9061),\n",
       " ('out', 8979),\n",
       " ('what', 8422),\n",
       " ('some', 8306),\n",
       " ('no', 8143),\n",
       " ('her', 7947),\n",
       " ('even', 7687),\n",
       " ('can', 7653),\n",
       " ('has', 7604),\n",
       " ('good', 7423),\n",
       " ('bad', 7401),\n",
       " ('would', 7036),\n",
       " ('up', 6970),\n",
       " ('only', 6781),\n",
       " ('more', 6730),\n",
       " ('when', 6726),\n",
       " ('she', 6444),\n",
       " ('really', 6262),\n",
       " ('time', 6209),\n",
       " ('had', 6142),\n",
       " ('my', 6015),\n",
       " ('were', 6001),\n",
       " ('which', 5780),\n",
       " ('very', 5764),\n",
       " ('me', 5606),\n",
       " ('see', 5452),\n",
       " ('don', 5336),\n",
       " ('we', 5328),\n",
       " ('their', 5278),\n",
       " ('do', 5236),\n",
       " ('story', 5208),\n",
       " ('than', 5183),\n",
       " ('been', 5100),\n",
       " ('much', 5078),\n",
       " ('get', 5037),\n",
       " ('because', 4966),\n",
       " ('people', 4806),\n",
       " ('then', 4761),\n",
       " ('make', 4722),\n",
       " ('how', 4688),\n",
       " ('could', 4686),\n",
       " ('any', 4658),\n",
       " ('into', 4567),\n",
       " ('made', 4541),\n",
       " ('first', 4306),\n",
       " ('other', 4305),\n",
       " ('well', 4254),\n",
       " ('too', 4174),\n",
       " ('them', 4165),\n",
       " ('plot', 4154),\n",
       " ('movies', 4080),\n",
       " ('acting', 4056),\n",
       " ('will', 3993),\n",
       " ('way', 3989),\n",
       " ('most', 3919),\n",
       " ('him', 3858),\n",
       " ('after', 3838),\n",
       " ('its', 3655),\n",
       " ('think', 3643),\n",
       " ('also', 3608),\n",
       " ('characters', 3600),\n",
       " ('off', 3567),\n",
       " ('watch', 3550),\n",
       " ('character', 3506),\n",
       " ('did', 3506),\n",
       " ('why', 3463),\n",
       " ('being', 3393),\n",
       " ('better', 3358),\n",
       " ('know', 3334),\n",
       " ('over', 3316),\n",
       " ('seen', 3265),\n",
       " ('ever', 3263),\n",
       " ('never', 3259),\n",
       " ('your', 3233),\n",
       " ('where', 3219),\n",
       " ('two', 3173),\n",
       " ('little', 3096),\n",
       " ('films', 3077),\n",
       " ('here', 3027),\n",
       " ('m', 3000),\n",
       " ('nothing', 2990),\n",
       " ('say', 2982),\n",
       " ('end', 2954),\n",
       " ('something', 2942),\n",
       " ('should', 2920),\n",
       " ('many', 2909),\n",
       " ('does', 2871),\n",
       " ('thing', 2866),\n",
       " ('show', 2862),\n",
       " ('ve', 2829),\n",
       " ('scene', 2816),\n",
       " ('scenes', 2785),\n",
       " ('these', 2724),\n",
       " ('go', 2717),\n",
       " ('didn', 2646),\n",
       " ('great', 2640),\n",
       " ('watching', 2640),\n",
       " ('re', 2620),\n",
       " ('doesn', 2601),\n",
       " ('through', 2560),\n",
       " ('such', 2544),\n",
       " ('man', 2516),\n",
       " ('worst', 2480),\n",
       " ('actually', 2449),\n",
       " ('actors', 2437),\n",
       " ('life', 2429),\n",
       " ('back', 2424),\n",
       " ('while', 2418),\n",
       " ('director', 2405),\n",
       " ('funny', 2336),\n",
       " ('going', 2319),\n",
       " ('still', 2283),\n",
       " ('another', 2254),\n",
       " ('look', 2247),\n",
       " ('now', 2237),\n",
       " ('old', 2215),\n",
       " ('those', 2212),\n",
       " ('real', 2170),\n",
       " ('few', 2158),\n",
       " ('love', 2152),\n",
       " ('horror', 2150),\n",
       " ('before', 2147),\n",
       " ('want', 2141),\n",
       " ('minutes', 2126),\n",
       " ('pretty', 2115),\n",
       " ('best', 2094),\n",
       " ('though', 2091),\n",
       " ('same', 2081),\n",
       " ('script', 2074),\n",
       " ('work', 2027),\n",
       " ('every', 2025),\n",
       " ('seems', 2023),\n",
       " ('least', 2011),\n",
       " ('enough', 1997),\n",
       " ('down', 1988),\n",
       " ('original', 1983),\n",
       " ('guy', 1964),\n",
       " ('got', 1952),\n",
       " ('around', 1943),\n",
       " ('part', 1942),\n",
       " ('lot', 1892),\n",
       " ('anything', 1874),\n",
       " ('find', 1860),\n",
       " ('new', 1854),\n",
       " ('again', 1849),\n",
       " ('isn', 1849),\n",
       " ('point', 1845),\n",
       " ('things', 1839),\n",
       " ('fact', 1839),\n",
       " ('give', 1823),\n",
       " ('makes', 1814),\n",
       " ('take', 1800),\n",
       " ('thought', 1798),\n",
       " ('d', 1770),\n",
       " ('whole', 1768),\n",
       " ('long', 1761),\n",
       " ('years', 1759),\n",
       " ('however', 1740),\n",
       " ('gets', 1714),\n",
       " ('making', 1695),\n",
       " ('cast', 1694),\n",
       " ('big', 1662),\n",
       " ('might', 1658),\n",
       " ('interesting', 1648),\n",
       " ('money', 1638),\n",
       " ('us', 1628),\n",
       " ('right', 1625),\n",
       " ('far', 1619),\n",
       " ('quite', 1596),\n",
       " ('without', 1595),\n",
       " ('come', 1595),\n",
       " ('almost', 1574),\n",
       " ('ll', 1567),\n",
       " ('action', 1566),\n",
       " ('awful', 1557),\n",
       " ('kind', 1539),\n",
       " ('reason', 1534),\n",
       " ('am', 1530),\n",
       " ('looks', 1528),\n",
       " ('must', 1522),\n",
       " ('done', 1510),\n",
       " ('comedy', 1504),\n",
       " ('someone', 1490),\n",
       " ('trying', 1486),\n",
       " ('wasn', 1484),\n",
       " ('poor', 1481),\n",
       " ('boring', 1478),\n",
       " ('instead', 1478),\n",
       " ('saw', 1475),\n",
       " ('away', 1469),\n",
       " ('girl', 1463),\n",
       " ('probably', 1444),\n",
       " ('believe', 1434),\n",
       " ('sure', 1433),\n",
       " ('looking', 1430),\n",
       " ('stupid', 1428),\n",
       " ('anyone', 1418),\n",
       " ('times', 1406),\n",
       " ('maybe', 1404),\n",
       " ('world', 1404),\n",
       " ('rather', 1394),\n",
       " ('terrible', 1391),\n",
       " ('may', 1390),\n",
       " ('last', 1390),\n",
       " ('since', 1388),\n",
       " ('let', 1385),\n",
       " ('tv', 1382),\n",
       " ('hard', 1374),\n",
       " ('between', 1374),\n",
       " ('waste', 1358),\n",
       " ('woman', 1356),\n",
       " ('feel', 1354),\n",
       " ('effects', 1348),\n",
       " ('half', 1341),\n",
       " ('own', 1333),\n",
       " ('young', 1317),\n",
       " ('music', 1316),\n",
       " ('idea', 1312),\n",
       " ('sense', 1306),\n",
       " ('bit', 1298),\n",
       " ('having', 1280),\n",
       " ('book', 1278),\n",
       " ('found', 1267),\n",
       " ('put', 1263),\n",
       " ('series', 1263),\n",
       " ('goes', 1256),\n",
       " ('worse', 1249),\n",
       " ('said', 1230),\n",
       " ('comes', 1224),\n",
       " ('role', 1222),\n",
       " ('main', 1220),\n",
       " ('else', 1199),\n",
       " ('everything', 1197),\n",
       " ('yet', 1196),\n",
       " ('low', 1189),\n",
       " ('screen', 1188),\n",
       " ('supposed', 1186),\n",
       " ('actor', 1185),\n",
       " ('either', 1183),\n",
       " ('budget', 1179),\n",
       " ('ending', 1179),\n",
       " ('audience', 1178),\n",
       " ('set', 1177),\n",
       " ('family', 1170),\n",
       " ('left', 1169),\n",
       " ('completely', 1168),\n",
       " ('both', 1158),\n",
       " ('wrong', 1155),\n",
       " ('always', 1151),\n",
       " ('course', 1148),\n",
       " ('place', 1148),\n",
       " ('seem', 1147),\n",
       " ('watched', 1142),\n",
       " ('day', 1132),\n",
       " ('simply', 1130),\n",
       " ('shot', 1126),\n",
       " ('mean', 1117),\n",
       " ('special', 1102),\n",
       " ('dead', 1101),\n",
       " ('three', 1094),\n",
       " ('house', 1085),\n",
       " ('oh', 1084),\n",
       " ('night', 1083),\n",
       " ('read', 1082),\n",
       " ('less', 1067),\n",
       " ('high', 1066),\n",
       " ('year', 1064),\n",
       " ('camera', 1061),\n",
       " ('worth', 1057),\n",
       " ('our', 1056),\n",
       " ('try', 1051),\n",
       " ('horrible', 1046),\n",
       " ('sex', 1046),\n",
       " ('video', 1043),\n",
       " ('black', 1039),\n",
       " ('although', 1036),\n",
       " ('couldn', 1036),\n",
       " ('once', 1033),\n",
       " ('rest', 1022),\n",
       " ('dvd', 1021),\n",
       " ('line', 1018),\n",
       " ('played', 1017),\n",
       " ('fun', 1007),\n",
       " ('during', 1006),\n",
       " ('production', 1003),\n",
       " ('everyone', 1002),\n",
       " ('play', 993),\n",
       " ('mind', 990),\n",
       " ('version', 989),\n",
       " ('kids', 989),\n",
       " ('seeing', 988),\n",
       " ('american', 980),\n",
       " ('given', 978),\n",
       " ('used', 969),\n",
       " ('performance', 968),\n",
       " ('especially', 963),\n",
       " ('together', 963),\n",
       " ('tell', 959),\n",
       " ('women', 958),\n",
       " ('start', 956),\n",
       " ('need', 955),\n",
       " ('second', 953),\n",
       " ('takes', 950),\n",
       " ('each', 950),\n",
       " ('wife', 944),\n",
       " ('dialogue', 942),\n",
       " ('use', 940),\n",
       " ('problem', 938),\n",
       " ('star', 934),\n",
       " ('unfortunately', 931),\n",
       " ('himself', 929),\n",
       " ('doing', 926),\n",
       " ('death', 922),\n",
       " ('name', 921),\n",
       " ('lines', 919),\n",
       " ('killer', 914),\n",
       " ('getting', 913),\n",
       " ('help', 905),\n",
       " ('couple', 902),\n",
       " ('fan', 902),\n",
       " ('head', 898),\n",
       " ('crap', 895),\n",
       " ('guess', 888),\n",
       " ('piece', 884),\n",
       " ('nice', 880),\n",
       " ('different', 878),\n",
       " ('school', 876),\n",
       " ('later', 875),\n",
       " ('entire', 869),\n",
       " ('shows', 860),\n",
       " ('next', 858),\n",
       " ('john', 858),\n",
       " ('short', 857),\n",
       " ('seemed', 857),\n",
       " ('hollywood', 850),\n",
       " ('home', 848),\n",
       " ('true', 846),\n",
       " ('person', 846),\n",
       " ('absolutely', 842),\n",
       " ('sort', 840),\n",
       " ('care', 839),\n",
       " ('understand', 836),\n",
       " ('plays', 835),\n",
       " ('felt', 834),\n",
       " ('written', 829),\n",
       " ('title', 828),\n",
       " ('men', 822),\n",
       " ('until', 821),\n",
       " ('flick', 816),\n",
       " ('decent', 815),\n",
       " ('face', 814),\n",
       " ('friends', 810),\n",
       " ('stars', 807),\n",
       " ('job', 807),\n",
       " ('case', 807),\n",
       " ('itself', 804),\n",
       " ('yes', 801),\n",
       " ('perhaps', 800),\n",
       " ('went', 797),\n",
       " ('wanted', 797),\n",
       " ('called', 796),\n",
       " ('annoying', 795),\n",
       " ('ridiculous', 790),\n",
       " ('tries', 790),\n",
       " ('laugh', 788),\n",
       " ('evil', 787),\n",
       " ('along', 786),\n",
       " ('top', 785),\n",
       " ('hour', 784),\n",
       " ('full', 783),\n",
       " ('came', 780),\n",
       " ('writing', 780),\n",
       " ('keep', 770),\n",
       " ('totally', 767),\n",
       " ('playing', 766),\n",
       " ('god', 765),\n",
       " ('won', 764),\n",
       " ('guys', 763),\n",
       " ('already', 762),\n",
       " ('gore', 757),\n",
       " ('direction', 748),\n",
       " ('save', 746),\n",
       " ('lost', 745),\n",
       " ('example', 744),\n",
       " ('sound', 742),\n",
       " ('war', 741),\n",
       " ('attempt', 735),\n",
       " ('car', 733),\n",
       " ('except', 733),\n",
       " ('moments', 732),\n",
       " ('blood', 732),\n",
       " ('obviously', 730),\n",
       " ('act', 729),\n",
       " ('remember', 728),\n",
       " ('kill', 727),\n",
       " ('truly', 726),\n",
       " ('white', 726),\n",
       " ('father', 726),\n",
       " ('b', 725),\n",
       " ('thinking', 720),\n",
       " ('ok', 716),\n",
       " ('finally', 716),\n",
       " ('turn', 711),\n",
       " ('quality', 701),\n",
       " ('lack', 698),\n",
       " ('style', 694),\n",
       " ('wouldn', 693),\n",
       " ('cheap', 691),\n",
       " ('none', 690),\n",
       " ('kid', 686),\n",
       " ('please', 686),\n",
       " ('boy', 685),\n",
       " ('seriously', 684),\n",
       " ('lead', 680),\n",
       " ('dull', 677),\n",
       " ('children', 676),\n",
       " ('starts', 675),\n",
       " ('stuff', 673),\n",
       " ('hope', 672),\n",
       " ('looked', 670),\n",
       " ('recommend', 669),\n",
       " ('under', 668),\n",
       " ('run', 667),\n",
       " ('killed', 667),\n",
       " ('enjoy', 666),\n",
       " ('others', 666),\n",
       " ('etc', 663),\n",
       " ('myself', 663),\n",
       " ('beginning', 662),\n",
       " ('girls', 662),\n",
       " ('against', 662),\n",
       " ('obvious', 660),\n",
       " ('small', 660),\n",
       " ('hell', 659),\n",
       " ('slow', 657),\n",
       " ('hand', 656),\n",
       " ('wonder', 652),\n",
       " ('lame', 652),\n",
       " ('becomes', 651),\n",
       " ('picture', 651),\n",
       " ('based', 650),\n",
       " ('early', 648),\n",
       " ('behind', 646),\n",
       " ('poorly', 644),\n",
       " ('avoid', 642),\n",
       " ('apparently', 640),\n",
       " ('complete', 640),\n",
       " ('happens', 639),\n",
       " ('anyway', 638),\n",
       " ('classic', 637),\n",
       " ('several', 636),\n",
       " ('despite', 635),\n",
       " ('certainly', 635),\n",
       " ('episode', 635),\n",
       " ('often', 631),\n",
       " ('cut', 630),\n",
       " ('writer', 630),\n",
       " ('mother', 628),\n",
       " ('predictable', 628),\n",
       " ('gave', 628),\n",
       " ('become', 627),\n",
       " ('close', 625),\n",
       " ('fans', 624),\n",
       " ('saying', 621),\n",
       " ('scary', 619),\n",
       " ('stop', 618),\n",
       " ('live', 618),\n",
       " ('wants', 617),\n",
       " ('self', 615),\n",
       " ('mr', 612),\n",
       " ('jokes', 611),\n",
       " ('friend', 611),\n",
       " ('cannot', 610),\n",
       " ('overall', 609),\n",
       " ('cinema', 604),\n",
       " ('child', 603),\n",
       " ('silly', 601),\n",
       " ('beautiful', 596),\n",
       " ('human', 595),\n",
       " ('expect', 594),\n",
       " ('liked', 593),\n",
       " ('happened', 592),\n",
       " ('bunch', 590),\n",
       " ('entertaining', 590),\n",
       " ('actress', 588),\n",
       " ('final', 588),\n",
       " ('says', 584),\n",
       " ('performances', 584),\n",
       " ('turns', 577),\n",
       " ('humor', 577),\n",
       " ('themselves', 576),\n",
       " ('eyes', 576),\n",
       " ('hours', 574),\n",
       " ('happen', 573),\n",
       " ('basically', 572),\n",
       " ('days', 572),\n",
       " ('running', 571),\n",
       " ('involved', 569),\n",
       " ('disappointed', 569),\n",
       " ('call', 569),\n",
       " ('directed', 568),\n",
       " ('group', 568),\n",
       " ('fight', 567),\n",
       " ('daughter', 566),\n",
       " ('talking', 566),\n",
       " ('body', 566),\n",
       " ('badly', 565),\n",
       " ('sorry', 565),\n",
       " ('throughout', 563),\n",
       " ('viewer', 563),\n",
       " ('yourself', 562),\n",
       " ('extremely', 562),\n",
       " ('interest', 561),\n",
       " ('heard', 561),\n",
       " ('violence', 561),\n",
       " ('shots', 559),\n",
       " ('side', 557),\n",
       " ('word', 556),\n",
       " ('art', 555),\n",
       " ('possible', 554),\n",
       " ('dark', 551),\n",
       " ('game', 551),\n",
       " ('hero', 550),\n",
       " ('alone', 549),\n",
       " ('son', 547),\n",
       " ('type', 547),\n",
       " ('leave', 547),\n",
       " ('gives', 546),\n",
       " ('parts', 546),\n",
       " ('single', 546),\n",
       " ('started', 545),\n",
       " ('female', 543),\n",
       " ('rating', 541),\n",
       " ('mess', 541),\n",
       " ('voice', 541),\n",
       " ('aren', 540),\n",
       " ('town', 540),\n",
       " ('drama', 538),\n",
       " ('definitely', 537),\n",
       " ('unless', 536),\n",
       " ('review', 534),\n",
       " ('effort', 533),\n",
       " ('weak', 533),\n",
       " ('able', 533),\n",
       " ('took', 531),\n",
       " ('non', 530),\n",
       " ('five', 530),\n",
       " ('matter', 529),\n",
       " ('usually', 529),\n",
       " ('michael', 528),\n",
       " ('feeling', 526),\n",
       " ('huge', 523),\n",
       " ('sequel', 522),\n",
       " ('soon', 521),\n",
       " ('exactly', 520),\n",
       " ('past', 519),\n",
       " ('turned', 518),\n",
       " ('police', 518),\n",
       " ('tried', 515),\n",
       " ('middle', 513),\n",
       " ('talent', 513),\n",
       " ('genre', 512),\n",
       " ('zombie', 510),\n",
       " ('ends', 509),\n",
       " ('history', 509),\n",
       " ('straight', 503),\n",
       " ('opening', 501),\n",
       " ('serious', 501),\n",
       " ('coming', 501),\n",
       " ('moment', 500),\n",
       " ('lives', 499),\n",
       " ('sad', 499),\n",
       " ('dialog', 498),\n",
       " ('particularly', 498),\n",
       " ('editing', 493),\n",
       " ('clearly', 492),\n",
       " ('beyond', 491),\n",
       " ('earth', 491),\n",
       " ('taken', 490),\n",
       " ('cool', 490),\n",
       " ('level', 489),\n",
       " ('dumb', 489),\n",
       " ('okay', 488),\n",
       " ('major', 487),\n",
       " ('fast', 485),\n",
       " ('premise', 485),\n",
       " ('joke', 484),\n",
       " ('stories', 484),\n",
       " ('wasted', 483),\n",
       " ('minute', 483),\n",
       " ('across', 482),\n",
       " ('mostly', 482),\n",
       " ('rent', 482),\n",
       " ('late', 481),\n",
       " ('falls', 481),\n",
       " ('fails', 481),\n",
       " ('mention', 478),\n",
       " ('theater', 475),\n",
       " ('stay', 472),\n",
       " ('sometimes', 472),\n",
       " ('hit', 468),\n",
       " ('talk', 467),\n",
       " ('fine', 467),\n",
       " ('die', 466),\n",
       " ('storyline', 465),\n",
       " ('pointless', 465),\n",
       " ('taking', 464),\n",
       " ('order', 462),\n",
       " ('brother', 461),\n",
       " ('whatever', 460),\n",
       " ('told', 460),\n",
       " ('wish', 458),\n",
       " ('room', 456),\n",
       " ('career', 455),\n",
       " ('appears', 455),\n",
       " ('write', 455),\n",
       " ('known', 454),\n",
       " ('husband', 454),\n",
       " ('living', 451),\n",
       " ('sit', 450),\n",
       " ('ten', 450),\n",
       " ('words', 449),\n",
       " ('monster', 448),\n",
       " ('chance', 448),\n",
       " ('hate', 444),\n",
       " ('novel', 444),\n",
       " ('add', 443),\n",
       " ('english', 443),\n",
       " ('somehow', 441),\n",
       " ('strange', 440),\n",
       " ('imdb', 438),\n",
       " ('actual', 438),\n",
       " ('total', 437),\n",
       " ('material', 437),\n",
       " ('killing', 437),\n",
       " ('ones', 437),\n",
       " ('knew', 436),\n",
       " ('king', 434),\n",
       " ('number', 434),\n",
       " ('using', 433),\n",
       " ('lee', 431),\n",
       " ('power', 431),\n",
       " ('shown', 431),\n",
       " ('works', 431),\n",
       " ('giving', 431),\n",
       " ('points', 430),\n",
       " ('possibly', 430),\n",
       " ('kept', 430),\n",
       " ('four', 429),\n",
       " ('local', 427),\n",
       " ('usual', 426),\n",
       " ('including', 425),\n",
       " ('problems', 424),\n",
       " ('ago', 424),\n",
       " ('opinion', 424),\n",
       " ('nudity', 423),\n",
       " ('age', 422),\n",
       " ('due', 421),\n",
       " ('roles', 420),\n",
       " ('writers', 419),\n",
       " ('decided', 419),\n",
       " ('near', 418),\n",
       " ('flat', 418),\n",
       " ('easily', 418),\n",
       " ('murder', 417),\n",
       " ('experience', 417),\n",
       " ('reviews', 416),\n",
       " ('imagine', 415),\n",
       " ('feels', 413),\n",
       " ('plain', 411),\n",
       " ('somewhat', 411),\n",
       " ('class', 410),\n",
       " ('score', 410),\n",
       " ('song', 409),\n",
       " ('bring', 409),\n",
       " ('whether', 409),\n",
       " ('otherwise', 408),\n",
       " ('whose', 408),\n",
       " ('average', 408),\n",
       " ('pathetic', 407),\n",
       " ('nearly', 407),\n",
       " ('knows', 407),\n",
       " ('zombies', 407),\n",
       " ('cinematography', 406),\n",
       " ('cheesy', 406),\n",
       " ('upon', 406),\n",
       " ('city', 405),\n",
       " ('space', 405),\n",
       " ('credits', 404),\n",
       " ('james', 403),\n",
       " ('lots', 403),\n",
       " ('change', 403),\n",
       " ('entertainment', 402),\n",
       " ('nor', 402),\n",
       " ('wait', 401),\n",
       " ('released', 400),\n",
       " ('needs', 399),\n",
       " ('shame', 398),\n",
       " ('attention', 396),\n",
       " ('comments', 394),\n",
       " ('bored', 393),\n",
       " ('free', 393),\n",
       " ('lady', 393),\n",
       " ('expected', 392),\n",
       " ('needed', 392),\n",
       " ('clear', 392),\n",
       " ('view', 391),\n",
       " ('development', 390),\n",
       " ('check', 390),\n",
       " ('doubt', 390),\n",
       " ('figure', 389),\n",
       " ('mystery', 389),\n",
       " ('excellent', 388),\n",
       " ('garbage', 388),\n",
       " ('sequence', 386),\n",
       " ('television', 386),\n",
       " ('o', 385),\n",
       " ('sets', 385),\n",
       " ('laughable', 384),\n",
       " ('potential', 384),\n",
       " ('robert', 382),\n",
       " ('light', 382),\n",
       " ('country', 382),\n",
       " ('documentary', 382),\n",
       " ('reality', 382),\n",
       " ('general', 381),\n",
       " ('ask', 381),\n",
       " ('comic', 380),\n",
       " ('fall', 380),\n",
       " ('begin', 380),\n",
       " ('footage', 379),\n",
       " ('stand', 379),\n",
       " ('forced', 379),\n",
       " ('trash', 379),\n",
       " ('remake', 379),\n",
       " ('thriller', 378),\n",
       " ('songs', 378),\n",
       " ('gay', 377),\n",
       " ('within', 377),\n",
       " ('hardly', 376),\n",
       " ('above', 375),\n",
       " ('gone', 375),\n",
       " ('george', 374),\n",
       " ('means', 373),\n",
       " ('sounds', 373),\n",
       " ('directing', 372),\n",
       " ('move', 372),\n",
       " ('david', 372),\n",
       " ('buy', 372),\n",
       " ('rock', 371),\n",
       " ('forward', 371),\n",
       " ('important', 371),\n",
       " ('hot', 370),\n",
       " ('haven', 370),\n",
       " ('filmed', 370),\n",
       " ('british', 370),\n",
       " ('heart', 369),\n",
       " ('reading', 369),\n",
       " ('fake', 369),\n",
       " ('incredibly', 368),\n",
       " ('weird', 368),\n",
       " ('hear', 368),\n",
       " ('enjoyed', 367),\n",
       " ('hilarious', 367),\n",
       " ('cop', 367),\n",
       " ('musical', 367),\n",
       " ('message', 366),\n",
       " ('happy', 366),\n",
       " ('pay', 366),\n",
       " ('laughs', 365),\n",
       " ('box', 365),\n",
       " ('suspense', 363),\n",
       " ('sadly', 363),\n",
       " ('eye', 362),\n",
       " ('third', 361),\n",
       " ('similar', 361),\n",
       " ('named', 361),\n",
       " ('modern', 360),\n",
       " ('failed', 359),\n",
       " ('events', 359),\n",
       " ('forget', 358),\n",
       " ('question', 358),\n",
       " ('male', 357),\n",
       " ('finds', 357),\n",
       " ('perfect', 356),\n",
       " ('spent', 355),\n",
       " ('sister', 355),\n",
       " ('feature', 354),\n",
       " ('result', 354),\n",
       " ('comment', 353),\n",
       " ('girlfriend', 353),\n",
       " ('sexual', 352),\n",
       " ('attempts', 351),\n",
       " ('neither', 351),\n",
       " ('richard', 351),\n",
       " ('screenplay', 350),\n",
       " ('elements', 350),\n",
       " ('spoilers', 349),\n",
       " ('brain', 348),\n",
       " ('filmmakers', 348),\n",
       " ('showing', 348),\n",
       " ('miss', 347),\n",
       " ('dr', 347),\n",
       " ('christmas', 347),\n",
       " ('cover', 345),\n",
       " ('red', 344),\n",
       " ('sequences', 344),\n",
       " ('typical', 343),\n",
       " ('excuse', 343),\n",
       " ('crazy', 342),\n",
       " ('ideas', 342),\n",
       " ('baby', 342),\n",
       " ('loved', 341),\n",
       " ('meant', 341),\n",
       " ('worked', 340),\n",
       " ('fire', 340),\n",
       " ('unbelievable', 339),\n",
       " ('follow', 339),\n",
       " ('theme', 337),\n",
       " ('barely', 336),\n",
       " ('producers', 336),\n",
       " ('twist', 336),\n",
       " ('plus', 336),\n",
       " ('appear', 336),\n",
       " ('directors', 335),\n",
       " ('team', 335),\n",
       " ('viewers', 333),\n",
       " ('leads', 332),\n",
       " ('tom', 332),\n",
       " ('slasher', 332),\n",
       " ('wrote', 331),\n",
       " ('villain', 331),\n",
       " ('gun', 331),\n",
       " ('working', 331),\n",
       " ('island', 330),\n",
       " ('strong', 330),\n",
       " ('open', 330),\n",
       " ('realize', 330),\n",
       " ('positive', 329),\n",
       " ('disappointing', 329),\n",
       " ('yeah', 329),\n",
       " ('quickly', 329),\n",
       " ('weren', 328),\n",
       " ('release', 328),\n",
       " ('simple', 328),\n",
       " ('honestly', 328),\n",
       " ('eventually', 327),\n",
       " ('period', 327),\n",
       " ('tells', 327),\n",
       " ('kills', 327),\n",
       " ('doctor', 327),\n",
       " ('nowhere', 326),\n",
       " ('list', 326),\n",
       " ('acted', 326),\n",
       " ('herself', 326),\n",
       " ('dog', 326),\n",
       " ('walk', 325),\n",
       " ('air', 324),\n",
       " ('apart', 324),\n",
       " ('makers', 323),\n",
       " ('subject', 323),\n",
       " ('learn', 322),\n",
       " ('fi', 322),\n",
       " ('sci', 319),\n",
       " ('bother', 319),\n",
       " ('admit', 319),\n",
       " ('jack', 318),\n",
       " ('disappointment', 318),\n",
       " ('hands', 318),\n",
       " ('note', 318),\n",
       " ('certain', 317),\n",
       " ('e', 317),\n",
       " ('value', 317),\n",
       " ('casting', 317),\n",
       " ('grade', 316),\n",
       " ('peter', 316),\n",
       " ('suddenly', 315),\n",
       " ('missing', 315),\n",
       " ('form', 313),\n",
       " ('stick', 313),\n",
       " ('previous', 313),\n",
       " ('break', 313),\n",
       " ('soundtrack', 312),\n",
       " ('surprised', 311),\n",
       " ('front', 311),\n",
       " ('expecting', 311),\n",
       " ('parents', 310),\n",
       " ('surprise', 310),\n",
       " ('relationship', 310),\n",
       " ('shoot', 309),\n",
       " ('today', 309),\n",
       " ('painful', 308),\n",
       " ('ways', 308),\n",
       " ('leaves', 308),\n",
       " ('ended', 308),\n",
       " ('creepy', 308),\n",
       " ('concept', 308),\n",
       " ('somewhere', 308),\n",
       " ('vampire', 308),\n",
       " ('spend', 307),\n",
       " ('th', 307),\n",
       " ('future', 306),\n",
       " ('difficult', 306),\n",
       " ('effect', 306),\n",
       " ('fighting', 306),\n",
       " ('street', 306),\n",
       " ('c', 305),\n",
       " ('america', 305),\n",
       " ('accent', 304),\n",
       " ('truth', 302),\n",
       " ('project', 302),\n",
       " ('joe', 301),\n",
       " ('f', 301),\n",
       " ('deal', 301),\n",
       " ('indeed', 301),\n",
       " ('biggest', 300),\n",
       " ('rate', 300),\n",
       " ('paul', 299),\n",
       " ('japanese', 299),\n",
       " ('utterly', 298),\n",
       " ('begins', 298),\n",
       " ('redeeming', 298),\n",
       " ('college', 298),\n",
       " ('york', 297),\n",
       " ('fairly', 297),\n",
       " ('disney', 297),\n",
       " ('crew', 296),\n",
       " ('create', 296),\n",
       " ('cartoon', 296),\n",
       " ('revenge', 296),\n",
       " ('co', 295),\n",
       " ('outside', 295),\n",
       " ('computer', 295),\n",
       " ('interested', 295),\n",
       " ('stage', 295),\n",
       " ('considering', 294),\n",
       " ('speak', 294),\n",
       " ('among', 294),\n",
       " ('towards', 293),\n",
       " ('channel', 293),\n",
       " ('sick', 293),\n",
       " ('talented', 292),\n",
       " ('cause', 292),\n",
       " ('particular', 292),\n",
       " ('van', 292),\n",
       " ('hair', 292),\n",
       " ('bottom', 291),\n",
       " ('reasons', 291),\n",
       " ('mediocre', 290),\n",
       " ('cat', 290),\n",
       " ('telling', 290),\n",
       " ('supporting', 289),\n",
       " ('store', 289),\n",
       " ('hoping', 288),\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for term, count in list(total_count.most_common()):\n",
    "    if(count > 100):\n",
    "        pos_neg_ratio = positive_count[term] / (negative_count[term] + 1)\n",
    "        pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "for word, ratio in pos_neg_ratios.most_common():\n",
    "    if(ratio > 1):\n",
    "        pos_neg_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        pos_neg_ratios[word] = - np.log(1/(ratio+0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edie', 4.6913478822291435),\n",
       " ('paulie', 4.0775374439057197),\n",
       " ('felix', 3.1527360223636558),\n",
       " ('polanski', 2.8233610476132043),\n",
       " ('matthau', 2.8067217286092401),\n",
       " ('victoria', 2.6810215287142909),\n",
       " ('mildred', 2.6026896854443837),\n",
       " ('gandhi', 2.5389738710582761),\n",
       " ('flawless', 2.451005098112319),\n",
       " ('superbly', 2.2600254785752498),\n",
       " ('perfection', 2.1594842493533721),\n",
       " ('astaire', 2.1400661634962708),\n",
       " ('captures', 2.0386195471595809),\n",
       " ('voight', 2.0301704926730531),\n",
       " ('wonderfully', 2.0218960560332353),\n",
       " ('powell', 1.9783454248084671),\n",
       " ('brosnan', 1.9547990964725592),\n",
       " ('lily', 1.9203768470501485),\n",
       " ('bakshi', 1.9029851043382795),\n",
       " ('lincoln', 1.9014583864844796),\n",
       " ('refreshing', 1.8551812956655511),\n",
       " ('breathtaking', 1.8481124057791867),\n",
       " ('bourne', 1.8478489358790986),\n",
       " ('lemmon', 1.8458266904983307),\n",
       " ('delightful', 1.8002701588959635),\n",
       " ('flynn', 1.7996646487351682),\n",
       " ('andrews', 1.7764919970972666),\n",
       " ('homer', 1.7692866133759964),\n",
       " ('beautifully', 1.7626953362841438),\n",
       " ('soccer', 1.7578579175523736)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_ratios.most_common()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boll', -4.0778152602708904),\n",
       " ('uwe', -3.9218753018711578),\n",
       " ('seagal', -3.3202501058581921),\n",
       " ('unwatchable', -3.0269848170580955),\n",
       " ('stinker', -2.9876839403711624),\n",
       " ('mst', -2.7753833211707968),\n",
       " ('incoherent', -2.7641396677532537),\n",
       " ('unfunny', -2.5545257844967644),\n",
       " ('waste', -2.4907515123361046),\n",
       " ('blah', -2.4475792789485005),\n",
       " ('horrid', -2.3715779644809971),\n",
       " ('pointless', -2.3451073877136341),\n",
       " ('atrocious', -2.3187369339642556),\n",
       " ('redeeming', -2.2667790015910296),\n",
       " ('prom', -2.2601040980178784),\n",
       " ('drivel', -2.2476029585766928),\n",
       " ('lousy', -2.2118080125207054),\n",
       " ('worst', -2.1930856334332267),\n",
       " ('laughable', -2.172468615469592),\n",
       " ('awful', -2.1385076866397488),\n",
       " ('poorly', -2.1326133844207011),\n",
       " ('wasting', -2.1178155545614512),\n",
       " ('remotely', -2.111046881095167),\n",
       " ('existent', -2.0024805005437076),\n",
       " ('boredom', -1.9241486572738005),\n",
       " ('miserably', -1.9216610938019989),\n",
       " ('sucks', -1.9166645809588516),\n",
       " ('uninspired', -1.9131499212248517),\n",
       " ('lame', -1.9117232884159072),\n",
       " ('insult', -1.9085323769376259)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(pos_neg_ratios.most_common()))[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74075\n"
     ]
    }
   ],
   "source": [
    "vocab = set(total_count.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create empty vector\n",
    "import numpy as np\n",
    "layer_0 = np.zeros((1,vocab_size))\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'lawston': 1,\n",
       " 'ursine': 2,\n",
       " 'rajnikant': 3,\n",
       " 'piovani': 4,\n",
       " 'tomi': 5,\n",
       " 'dependence': 6,\n",
       " 'haigh': 7,\n",
       " 'enjoyably': 8,\n",
       " 'voyage': 9,\n",
       " 'disrepair': 10,\n",
       " 'convicting': 11,\n",
       " 'lesley': 12,\n",
       " 'capraesque': 13,\n",
       " 'dislike': 14,\n",
       " 'truax': 15,\n",
       " 'gangu': 16,\n",
       " 'hughly': 17,\n",
       " 'gushy': 18,\n",
       " 'rhidian': 19,\n",
       " 'empires': 20,\n",
       " 'laughter': 21,\n",
       " 'golgo': 22,\n",
       " 'cadence': 23,\n",
       " 'freaked': 24,\n",
       " 'macnicol': 25,\n",
       " 'decisive': 26,\n",
       " 'shek': 27,\n",
       " 'meridian': 28,\n",
       " 'earhole': 29,\n",
       " 'eleniak': 30,\n",
       " 'dis': 31,\n",
       " 'nonstop': 32,\n",
       " 'medicalgenetic': 33,\n",
       " 'perrineau': 34,\n",
       " 'elviras': 35,\n",
       " 'railway': 36,\n",
       " 'dutched': 37,\n",
       " 'clone': 38,\n",
       " 'homely': 39,\n",
       " 'knighthood': 40,\n",
       " 'waft': 41,\n",
       " 'peak': 42,\n",
       " 'uninvolved': 43,\n",
       " 'instigators': 44,\n",
       " 'marksmanship': 45,\n",
       " 'neck': 46,\n",
       " 'owls': 47,\n",
       " 'cf': 48,\n",
       " 'district': 49,\n",
       " 'spock': 50,\n",
       " 'biographies': 51,\n",
       " 'uhura': 52,\n",
       " 'prepaid': 53,\n",
       " 'phenoms': 54,\n",
       " 'burgundians': 55,\n",
       " 'death': 56,\n",
       " 'nra': 57,\n",
       " 'shirley': 58,\n",
       " 'shelf': 59,\n",
       " 'bleed': 60,\n",
       " 'camerawith': 61,\n",
       " 'indigestible': 62,\n",
       " 'daens': 63,\n",
       " 'acclimation': 64,\n",
       " 'touting': 65,\n",
       " 'margot': 66,\n",
       " 'saul': 67,\n",
       " 'cessna': 68,\n",
       " 'suit': 69,\n",
       " 'albaladejo': 70,\n",
       " 'betsy': 71,\n",
       " 'beaded': 72,\n",
       " 'frequently': 73,\n",
       " 'sprinting': 74,\n",
       " 'indra': 75,\n",
       " 'daniella': 76,\n",
       " 'defendants': 77,\n",
       " 'dispite': 78,\n",
       " 'wisecracking': 79,\n",
       " 'crochet': 80,\n",
       " 'fashioned': 81,\n",
       " 'lynx': 82,\n",
       " 'anticompetitive': 83,\n",
       " 'joshua': 84,\n",
       " 'obscene': 85,\n",
       " 'millinium': 86,\n",
       " 'contention': 87,\n",
       " 'kayaks': 88,\n",
       " 'timberflake': 89,\n",
       " 'hai': 90,\n",
       " 'ioan': 91,\n",
       " 'bagging': 92,\n",
       " 'y': 93,\n",
       " 'scavengers': 94,\n",
       " 'prier': 95,\n",
       " 'kato': 96,\n",
       " 'recrudescence': 97,\n",
       " 'phenolic': 98,\n",
       " 'chaptered': 99,\n",
       " 'sweeny': 100,\n",
       " 'benvolio': 101,\n",
       " 'classic': 102,\n",
       " 'sinned': 103,\n",
       " 'rejected': 104,\n",
       " 'tuvok': 105,\n",
       " 'funerals': 106,\n",
       " 'monogamy': 107,\n",
       " 'hairdewed': 108,\n",
       " 'viard': 109,\n",
       " 'clearances': 110,\n",
       " 'replace': 111,\n",
       " 'polemicist': 112,\n",
       " 'worldy': 113,\n",
       " 'emphysema': 114,\n",
       " 'makin': 115,\n",
       " 'cab': 116,\n",
       " 'facials': 117,\n",
       " 'jorma': 118,\n",
       " 'luthercorp': 119,\n",
       " 'kel': 120,\n",
       " 'antagonist': 121,\n",
       " 'sage': 122,\n",
       " 'inflicts': 123,\n",
       " 'titted': 124,\n",
       " 'thomilson': 125,\n",
       " 'redundanteven': 126,\n",
       " 'espionage': 127,\n",
       " 'outsourcing': 128,\n",
       " 'aidan': 129,\n",
       " 'dover': 130,\n",
       " 'infinite': 131,\n",
       " 'swang': 132,\n",
       " 'arjuna': 133,\n",
       " 'guidlines': 134,\n",
       " 'theroux': 135,\n",
       " 'casnoff': 136,\n",
       " 'tuttle': 137,\n",
       " 'unrestricted': 138,\n",
       " 'cahiil': 139,\n",
       " 'susceptible': 140,\n",
       " 'madder': 141,\n",
       " 'mccall': 142,\n",
       " 'gourmets': 143,\n",
       " 'jell': 144,\n",
       " 'indignantly': 145,\n",
       " 'severe': 146,\n",
       " 'flaunted': 147,\n",
       " 'juggles': 148,\n",
       " 'braintrust': 149,\n",
       " 'bombings': 150,\n",
       " 'tundra': 151,\n",
       " 'unkillable': 152,\n",
       " 'reglamentary': 153,\n",
       " 'rafi': 154,\n",
       " 'miami': 155,\n",
       " 'mckellen': 156,\n",
       " 'cinematicism': 157,\n",
       " 'rawls': 158,\n",
       " 'riverboat': 159,\n",
       " 'acceleration': 160,\n",
       " 'irish': 161,\n",
       " 'lettuce': 162,\n",
       " 'policewoman': 163,\n",
       " 'bewildered': 164,\n",
       " 'sensual': 165,\n",
       " 'kimi': 166,\n",
       " 'ficker': 167,\n",
       " 'geraldine': 168,\n",
       " 'departments': 169,\n",
       " 'clare': 170,\n",
       " 'groot': 171,\n",
       " 'vindicated': 172,\n",
       " 'guidelines': 173,\n",
       " 'tarka': 174,\n",
       " 'schertler': 175,\n",
       " 'dillusion': 176,\n",
       " 'rashness': 177,\n",
       " 'pinkish': 178,\n",
       " 'liberated': 179,\n",
       " 'miraculously': 180,\n",
       " 'imelda': 181,\n",
       " 'navuoo': 182,\n",
       " 'hommage': 183,\n",
       " 'mule': 184,\n",
       " 'bergammi': 185,\n",
       " 'mortem': 186,\n",
       " 'twit': 187,\n",
       " 'nelly': 188,\n",
       " 'wwwwhhhyyyyyyy': 189,\n",
       " 'unger': 190,\n",
       " 'gaming': 191,\n",
       " 'undone': 192,\n",
       " 'overcoats': 193,\n",
       " 'marlon': 194,\n",
       " 'mustache': 195,\n",
       " 'horridly': 196,\n",
       " 'shiven': 197,\n",
       " 'analogical': 198,\n",
       " 'screwdrivers': 199,\n",
       " 'abiding': 200,\n",
       " 'discrimination': 201,\n",
       " 'innovative': 202,\n",
       " 'bch': 203,\n",
       " 'putdowns': 204,\n",
       " 'montaged': 205,\n",
       " 'greenland': 206,\n",
       " 'nitu': 207,\n",
       " 'ef': 208,\n",
       " 'zig': 209,\n",
       " 'bete': 210,\n",
       " 'choices': 211,\n",
       " 'hitlerthe': 212,\n",
       " 'enlivenes': 213,\n",
       " 'palate': 214,\n",
       " 'sjman': 215,\n",
       " 'nervosa': 216,\n",
       " 'teetered': 217,\n",
       " 'briton': 218,\n",
       " 'archibald': 219,\n",
       " 'tobacco': 220,\n",
       " 'talks': 221,\n",
       " 'deficiencies': 222,\n",
       " 'favored': 223,\n",
       " 'blackberry': 224,\n",
       " 'conor': 225,\n",
       " 'bissau': 226,\n",
       " 'werewolfs': 227,\n",
       " 'jmv': 228,\n",
       " 'denemark': 229,\n",
       " 'alabaster': 230,\n",
       " 'operate': 231,\n",
       " 'cleverer': 232,\n",
       " 'nostalgic': 233,\n",
       " 'smuggling': 234,\n",
       " 'songling': 235,\n",
       " 'hiss': 236,\n",
       " 'daysthis': 237,\n",
       " 'spacecrafts': 238,\n",
       " 'singled': 239,\n",
       " 'olajima': 240,\n",
       " 'marquise': 241,\n",
       " 'upn': 242,\n",
       " 'resurfacing': 243,\n",
       " 'oopps': 244,\n",
       " 'chicatillo': 245,\n",
       " 'swelling': 246,\n",
       " 'dern': 247,\n",
       " 'brammell': 248,\n",
       " 'busts': 249,\n",
       " 'ram': 250,\n",
       " 'frozen': 251,\n",
       " 'hypothesis': 252,\n",
       " 'earpeircing': 253,\n",
       " 'bandannas': 254,\n",
       " 'malecio': 255,\n",
       " 'vollins': 256,\n",
       " 'krissakes': 257,\n",
       " 'cagliostro': 258,\n",
       " 'artfulness': 259,\n",
       " 'eyepatch': 260,\n",
       " 'midge': 261,\n",
       " 'sulu': 262,\n",
       " 'desperatelyy': 263,\n",
       " 'suggestions': 264,\n",
       " 'yeiks': 265,\n",
       " 'debit': 266,\n",
       " 'amitabh': 267,\n",
       " 'doesn': 268,\n",
       " 'mildest': 269,\n",
       " 'wrangles': 270,\n",
       " 'gegen': 271,\n",
       " 'montagues': 272,\n",
       " 'relations': 273,\n",
       " 'geology': 274,\n",
       " 'thanx': 275,\n",
       " 'frisson': 276,\n",
       " 'docteur': 277,\n",
       " 'firmware': 278,\n",
       " 'steinauer': 279,\n",
       " 'temperamental': 280,\n",
       " 'comes': 281,\n",
       " 'astutely': 282,\n",
       " 'obstinacy': 283,\n",
       " 'economical': 284,\n",
       " 'thionite': 285,\n",
       " 'montreux': 286,\n",
       " 'puhleasssssee': 287,\n",
       " 'relentlessy': 288,\n",
       " 'barings': 289,\n",
       " 'moves': 290,\n",
       " 'mccheese': 291,\n",
       " 'amateuristic': 292,\n",
       " 'opaeras': 293,\n",
       " 'nrnberg': 294,\n",
       " 'ayesha': 295,\n",
       " 'deedlit': 296,\n",
       " 'penalized': 297,\n",
       " 'languished': 298,\n",
       " 'courtyard': 299,\n",
       " 'nudes': 300,\n",
       " 'romances': 301,\n",
       " 'levelheadedness': 302,\n",
       " 'chrissakes': 303,\n",
       " 'infective': 304,\n",
       " 'goners': 305,\n",
       " 'noodled': 306,\n",
       " 'comparisons': 307,\n",
       " 'databanks': 308,\n",
       " 'airlessness': 309,\n",
       " 'psychotherapy': 310,\n",
       " 'hydroponics': 311,\n",
       " 'bamboo': 312,\n",
       " 'lucy': 313,\n",
       " 'coattails': 314,\n",
       " 'barnyard': 315,\n",
       " 'idiocy': 316,\n",
       " 'hollywoodand': 317,\n",
       " 'tailer': 318,\n",
       " 'vitriolic': 319,\n",
       " 'wormtong': 320,\n",
       " 'falsetto': 321,\n",
       " 'rarest': 322,\n",
       " 'bony': 323,\n",
       " 'wits': 324,\n",
       " 'sure': 325,\n",
       " 'interislander': 326,\n",
       " 'encapsulation': 327,\n",
       " 'toons': 328,\n",
       " 'honcho': 329,\n",
       " 'zaps': 330,\n",
       " 'zouganelis': 331,\n",
       " 'predecessorsovernight': 332,\n",
       " 'aero': 333,\n",
       " 'colossally': 334,\n",
       " 'terrifying': 335,\n",
       " 'blurred': 336,\n",
       " 'stitch': 337,\n",
       " 'honoria': 338,\n",
       " 'orifices': 339,\n",
       " 'dracko': 340,\n",
       " 'dood': 341,\n",
       " 'skyscrapers': 342,\n",
       " 'grace': 343,\n",
       " 'gulzar': 344,\n",
       " 'aloofness': 345,\n",
       " 'retitled': 346,\n",
       " 'sophie': 347,\n",
       " 'reverent': 348,\n",
       " 'glumly': 349,\n",
       " 'cheaters': 350,\n",
       " 'uninspiring': 351,\n",
       " 'benetakos': 352,\n",
       " 'demonstrate': 353,\n",
       " 'crapness': 354,\n",
       " 'ishwar': 355,\n",
       " 'proddings': 356,\n",
       " 'trivilized': 357,\n",
       " 'wiring': 358,\n",
       " 'pleases': 359,\n",
       " 'implausable': 360,\n",
       " 'satanically': 361,\n",
       " 'absorption': 362,\n",
       " 'booting': 363,\n",
       " 'gainsay': 364,\n",
       " 'fanfictions': 365,\n",
       " 'laundering': 366,\n",
       " 'diction': 367,\n",
       " 'competitive': 368,\n",
       " 'subverted': 369,\n",
       " 'inconsequentiality': 370,\n",
       " 'tentacles': 371,\n",
       " 'litvak': 372,\n",
       " 'jivetalking': 373,\n",
       " 'thickener': 374,\n",
       " 'caffeinated': 375,\n",
       " 'tuition': 376,\n",
       " 'heronimo': 377,\n",
       " 'batmite': 378,\n",
       " 'visconti': 379,\n",
       " 'tojo': 380,\n",
       " 'kareeena': 381,\n",
       " 'funfare': 382,\n",
       " 'unnameable': 383,\n",
       " 'margolis': 384,\n",
       " 'resoundingly': 385,\n",
       " 'galiano': 386,\n",
       " 'sine': 387,\n",
       " 'yielded': 388,\n",
       " 'fortyish': 389,\n",
       " 'bolted': 390,\n",
       " 'vasty': 391,\n",
       " 'baths': 392,\n",
       " 'differs': 393,\n",
       " 'deodorant': 394,\n",
       " 'councellor': 395,\n",
       " 'semantics': 396,\n",
       " 'winey': 397,\n",
       " 'unturned': 398,\n",
       " 'dissectionist': 399,\n",
       " 'hideaki': 400,\n",
       " 'carving': 401,\n",
       " 'streamlines': 402,\n",
       " 'whisked': 403,\n",
       " 'elderly': 404,\n",
       " 'overexposed': 405,\n",
       " 'sensibly': 406,\n",
       " 'apologist': 407,\n",
       " 'anybody': 408,\n",
       " 'accouterments': 409,\n",
       " 'purest': 410,\n",
       " 'eightiesly': 411,\n",
       " 'gender': 412,\n",
       " 'paraszhanov': 413,\n",
       " 'withdrawing': 414,\n",
       " 'ds': 415,\n",
       " 'treehouse': 416,\n",
       " 'tasked': 417,\n",
       " 'thied': 418,\n",
       " 'harvested': 419,\n",
       " 'francoise': 420,\n",
       " 'whoopi': 421,\n",
       " 'clunker': 422,\n",
       " 'philosophical': 423,\n",
       " 'pub': 424,\n",
       " 'puny': 425,\n",
       " 'vacuity': 426,\n",
       " 'alphas': 427,\n",
       " 'denoted': 428,\n",
       " 'conventionsas': 429,\n",
       " 'elitist': 430,\n",
       " 'wrap': 431,\n",
       " 'scissors': 432,\n",
       " 'atherton': 433,\n",
       " 'handwork': 434,\n",
       " 'sickness': 435,\n",
       " 'outtake': 436,\n",
       " 'wound': 437,\n",
       " 'ontop': 438,\n",
       " 'enveloping': 439,\n",
       " 'general': 440,\n",
       " 'soaked': 441,\n",
       " 'turntable': 442,\n",
       " 'morbius': 443,\n",
       " 'mcgovern': 444,\n",
       " 'fascinates': 445,\n",
       " 'commited': 446,\n",
       " 'tsang': 447,\n",
       " 'regress': 448,\n",
       " 'receptionists': 449,\n",
       " 'foul': 450,\n",
       " 'staten': 451,\n",
       " 'coworker': 452,\n",
       " 'mates': 453,\n",
       " 'palusky': 454,\n",
       " 'graf': 455,\n",
       " 'dissociates': 456,\n",
       " 'extracted': 457,\n",
       " 'severed': 458,\n",
       " 'recapping': 459,\n",
       " 'acmetropolis': 460,\n",
       " 'powerweight': 461,\n",
       " 'knows': 462,\n",
       " 'desist': 463,\n",
       " 'watros': 464,\n",
       " 'outers': 465,\n",
       " 'tibet': 466,\n",
       " 'cmm': 467,\n",
       " 'flintstones': 468,\n",
       " 'runtime': 469,\n",
       " 'legrand': 470,\n",
       " 'joxs': 471,\n",
       " 'undertext': 472,\n",
       " 'cianelli': 473,\n",
       " 'sickeningly': 474,\n",
       " 'gloomily': 475,\n",
       " 'olander': 476,\n",
       " 'scrapyard': 477,\n",
       " 'bakhtyari': 478,\n",
       " 'ballbusting': 479,\n",
       " 'arne': 480,\n",
       " 'liberally': 481,\n",
       " 'benefices': 482,\n",
       " 'snoozefest': 483,\n",
       " 'paragon': 484,\n",
       " 'astounds': 485,\n",
       " 'driest': 486,\n",
       " 'philo': 487,\n",
       " 'eartha': 488,\n",
       " 'eion': 489,\n",
       " 'mork': 490,\n",
       " 'gras': 491,\n",
       " 'underexplained': 492,\n",
       " 'oc': 493,\n",
       " 'ritterkreuz': 494,\n",
       " 'dismissively': 495,\n",
       " 'homoerotica': 496,\n",
       " 'touchingly': 497,\n",
       " 'needs': 498,\n",
       " 'represenative': 499,\n",
       " 'cavanagh': 500,\n",
       " 'curved': 501,\n",
       " 'mba': 502,\n",
       " 'uschi': 503,\n",
       " 'eavesdrops': 504,\n",
       " 'wunderkinds': 505,\n",
       " 'curtin': 506,\n",
       " 'autocratic': 507,\n",
       " 'delays': 508,\n",
       " 'thelma': 509,\n",
       " 'reservation': 510,\n",
       " 'sparingly': 511,\n",
       " 'humaine': 512,\n",
       " 'arching': 513,\n",
       " 'teary': 514,\n",
       " 'crux': 515,\n",
       " 'talkovers': 516,\n",
       " 'tribbiani': 517,\n",
       " 'refreshments': 518,\n",
       " 'being': 519,\n",
       " 'resembling': 520,\n",
       " 'exploit': 521,\n",
       " 'ustashe': 522,\n",
       " 'ramu': 523,\n",
       " 'mundainly': 524,\n",
       " 'vrs': 525,\n",
       " 'visayan': 526,\n",
       " 'gallaga': 527,\n",
       " 'rode': 528,\n",
       " 'francis': 529,\n",
       " 'wolfpack': 530,\n",
       " 'forsee': 531,\n",
       " 'drill': 532,\n",
       " 'pokmon': 533,\n",
       " 'roadhouse': 534,\n",
       " 'moronov': 535,\n",
       " 'hotd': 536,\n",
       " 'thambi': 537,\n",
       " 'fuji': 538,\n",
       " 'rehumanization': 539,\n",
       " 'degobah': 540,\n",
       " 'gwynne': 541,\n",
       " 'impeccably': 542,\n",
       " 'urbanscapes': 543,\n",
       " 'detour': 544,\n",
       " 'conscripted': 545,\n",
       " 'avatar': 546,\n",
       " 'syndicate': 547,\n",
       " 'distinctive': 548,\n",
       " 'japery': 549,\n",
       " 'kells': 550,\n",
       " 'sloppy': 551,\n",
       " 'izod': 552,\n",
       " 'banal': 553,\n",
       " 'telepath': 554,\n",
       " 'edtv': 555,\n",
       " 'grips': 556,\n",
       " 'icu': 557,\n",
       " 'barbour': 558,\n",
       " 'constipated': 559,\n",
       " 'dialoque': 560,\n",
       " 'congo': 561,\n",
       " 'umptieth': 562,\n",
       " 'unmemorable': 563,\n",
       " 'matondkar': 564,\n",
       " 'beetch': 565,\n",
       " 'liberalism': 566,\n",
       " 'zukhov': 567,\n",
       " 'kinghtly': 568,\n",
       " 'swim': 569,\n",
       " 'fickle': 570,\n",
       " 'blouses': 571,\n",
       " 'interludes': 572,\n",
       " 'thuy': 573,\n",
       " 'pared': 574,\n",
       " 'milligans': 575,\n",
       " 'heino': 576,\n",
       " 'mods': 577,\n",
       " 'comprehensively': 578,\n",
       " 'deel': 579,\n",
       " 'dum': 580,\n",
       " 'hanks': 581,\n",
       " 'hooligans': 582,\n",
       " 'franziska': 583,\n",
       " 'shadow': 584,\n",
       " 'disagreeing': 585,\n",
       " 'suraj': 586,\n",
       " 'scorned': 587,\n",
       " 'epos': 588,\n",
       " 'brighter': 589,\n",
       " 'daumier': 590,\n",
       " 'formans': 591,\n",
       " 'rues': 592,\n",
       " 'maestro': 593,\n",
       " 'remodeled': 594,\n",
       " 'quibbles': 595,\n",
       " 'clot': 596,\n",
       " 'confab': 597,\n",
       " 'imparts': 598,\n",
       " 'babble': 599,\n",
       " 'gret': 600,\n",
       " 'poopers': 601,\n",
       " 'xo': 602,\n",
       " 'petronijevic': 603,\n",
       " 'moomins': 604,\n",
       " 'mostly': 605,\n",
       " 'kitschy': 606,\n",
       " 'reintroduced': 607,\n",
       " 'unpolitically': 608,\n",
       " 'overpraised': 609,\n",
       " 'cancellation': 610,\n",
       " 'donnersmarck': 611,\n",
       " 'scarf': 612,\n",
       " 'benidict': 613,\n",
       " 'minuted': 614,\n",
       " 'chiani': 615,\n",
       " 'bocabonita': 616,\n",
       " 'precipitously': 617,\n",
       " 'tar': 618,\n",
       " 'maximising': 619,\n",
       " 'coates': 620,\n",
       " 'seashell': 621,\n",
       " 'sleaziest': 622,\n",
       " 'shafted': 623,\n",
       " 'coronets': 624,\n",
       " 'mcmurphy': 625,\n",
       " 'crocodiles': 626,\n",
       " 'gallant': 627,\n",
       " 'inarguable': 628,\n",
       " 'fateless': 629,\n",
       " 'blurring': 630,\n",
       " 'eally': 631,\n",
       " 'emotional': 632,\n",
       " 'sanguinusa': 633,\n",
       " 'duckies': 634,\n",
       " 'paesan': 635,\n",
       " 'striked': 636,\n",
       " 'interloper': 637,\n",
       " 'tenderer': 638,\n",
       " 'vultures': 639,\n",
       " 'excavations': 640,\n",
       " 'intertitle': 641,\n",
       " 'rapprochement': 642,\n",
       " 'sketchily': 643,\n",
       " 'taduz': 644,\n",
       " 'unarguably': 645,\n",
       " 'grrrl': 646,\n",
       " 'megalopolis': 647,\n",
       " 'mentioning': 648,\n",
       " 'tsanders': 649,\n",
       " 'witt': 650,\n",
       " 'intermixed': 651,\n",
       " 'mogul': 652,\n",
       " 'interrogating': 653,\n",
       " 'yasnaya': 654,\n",
       " 'kuala': 655,\n",
       " 'whyfore': 656,\n",
       " 'mustaches': 657,\n",
       " 'bitch': 658,\n",
       " 'soothe': 659,\n",
       " 'wyeth': 660,\n",
       " 'misaki': 661,\n",
       " 'dongen': 662,\n",
       " 'dickish': 663,\n",
       " 'removal': 664,\n",
       " 'omnibus': 665,\n",
       " 'marmorstein': 666,\n",
       " 'computerised': 667,\n",
       " 'singling': 668,\n",
       " 'fledged': 669,\n",
       " 'augmented': 670,\n",
       " 'facism': 671,\n",
       " 'suavely': 672,\n",
       " 'denman': 673,\n",
       " 'acquaintaces': 674,\n",
       " 'descript': 675,\n",
       " 'denomination': 676,\n",
       " 'stage': 677,\n",
       " 'piddling': 678,\n",
       " 'opinionated': 679,\n",
       " 'warnes': 680,\n",
       " 'walking': 681,\n",
       " 'nibby': 682,\n",
       " 'heidecke': 683,\n",
       " 'purposly': 684,\n",
       " 'manero': 685,\n",
       " 'dung': 686,\n",
       " 'tutorial': 687,\n",
       " 'uneasy': 688,\n",
       " 'kucch': 689,\n",
       " 'wise': 690,\n",
       " 'liquidates': 691,\n",
       " 'bletchly': 692,\n",
       " 'silicons': 693,\n",
       " 'katsuhiro': 694,\n",
       " 'jalouse': 695,\n",
       " 'hattori': 696,\n",
       " 'consensus': 697,\n",
       " 'unemotional': 698,\n",
       " 'tragedies': 699,\n",
       " 'leaping': 700,\n",
       " 'piloted': 701,\n",
       " 'repentant': 702,\n",
       " 'swanton': 703,\n",
       " 'pupil': 704,\n",
       " 'kaempfen': 705,\n",
       " 'protoplasms': 706,\n",
       " 'foch': 707,\n",
       " 'huckabees': 708,\n",
       " 'pelham': 709,\n",
       " 'wingtip': 710,\n",
       " 'nuyen': 711,\n",
       " 'exchanged': 712,\n",
       " 'gourgous': 713,\n",
       " 'revealing': 714,\n",
       " 'brokers': 715,\n",
       " 'jawed': 716,\n",
       " 'akte': 717,\n",
       " 'functionality': 718,\n",
       " 'baddiel': 719,\n",
       " 'parents': 720,\n",
       " 'windscreen': 721,\n",
       " 'satirical': 722,\n",
       " 'buckaroo': 723,\n",
       " 'satisfactorily': 724,\n",
       " 'hava': 725,\n",
       " 'karaindrou': 726,\n",
       " 'created': 727,\n",
       " 'thunderstruck': 728,\n",
       " 'migraines': 729,\n",
       " 'chopsticks': 730,\n",
       " 'quadraphenia': 731,\n",
       " 'natsu': 732,\n",
       " 'definently': 733,\n",
       " 'sk': 734,\n",
       " 'forefront': 735,\n",
       " 'intercepted': 736,\n",
       " 'krabb': 737,\n",
       " 'alarmists': 738,\n",
       " 'madchenjahre': 739,\n",
       " 'dorsal': 740,\n",
       " 'suggestible': 741,\n",
       " 'kyra': 742,\n",
       " 'spot': 743,\n",
       " 'cobern': 744,\n",
       " 'unknowns': 745,\n",
       " 'tupi': 746,\n",
       " 'arbaaz': 747,\n",
       " 'transformations': 748,\n",
       " 'hanna': 749,\n",
       " 'mesmerising': 750,\n",
       " 'unthoughtful': 751,\n",
       " 'buttocks': 752,\n",
       " 'gad': 753,\n",
       " 'originator': 754,\n",
       " 'efficiency': 755,\n",
       " 'preeners': 756,\n",
       " 'ehle': 757,\n",
       " 'ridicoulus': 758,\n",
       " 'graystone': 759,\n",
       " 'dimensions': 760,\n",
       " 'disappointmented': 761,\n",
       " 'robbins': 762,\n",
       " 'excursionists': 763,\n",
       " 'oppressors': 764,\n",
       " 'dei': 765,\n",
       " 'gay': 766,\n",
       " 'chubby': 767,\n",
       " 'rankers': 768,\n",
       " 'primitives': 769,\n",
       " 'mayne': 770,\n",
       " 'fossils': 771,\n",
       " 'furlough': 772,\n",
       " 'narcotics': 773,\n",
       " 'conceive': 774,\n",
       " 'filmographies': 775,\n",
       " 'rai': 776,\n",
       " 'waco': 777,\n",
       " 'apodictic': 778,\n",
       " 'charactersthe': 779,\n",
       " 'loud': 780,\n",
       " 'lad': 781,\n",
       " 'grod': 782,\n",
       " 'mendocino': 783,\n",
       " 'symbols': 784,\n",
       " 'riggs': 785,\n",
       " 'triers': 786,\n",
       " 'timber': 787,\n",
       " 'unworkable': 788,\n",
       " 'urbania': 789,\n",
       " 'ignorant': 790,\n",
       " 'freakout': 791,\n",
       " 'foils': 792,\n",
       " 'boyars': 793,\n",
       " 'inyong': 794,\n",
       " 'unapologetically': 795,\n",
       " 'exaggeratedly': 796,\n",
       " 'undermines': 797,\n",
       " 'jidai': 798,\n",
       " 'quelle': 799,\n",
       " 'roeh': 800,\n",
       " 'bigbossman': 801,\n",
       " 'bombasticities': 802,\n",
       " 'annivesery': 803,\n",
       " 'hopped': 804,\n",
       " 'bilko': 805,\n",
       " 'weinstein': 806,\n",
       " 'honours': 807,\n",
       " 'rummaged': 808,\n",
       " 'fools': 809,\n",
       " 'sporadically': 810,\n",
       " 'outburst': 811,\n",
       " 'toxie': 812,\n",
       " 'pardes': 813,\n",
       " 'tenor': 814,\n",
       " 'mismatch': 815,\n",
       " 'inclusive': 816,\n",
       " 'kol': 817,\n",
       " 'crouching': 818,\n",
       " 'bourvier': 819,\n",
       " 'bodes': 820,\n",
       " 'washington': 821,\n",
       " 'celoron': 822,\n",
       " 'sanam': 823,\n",
       " 'mistakenly': 824,\n",
       " 'jaclyn': 825,\n",
       " 'anticipatory': 826,\n",
       " 'colton': 827,\n",
       " 'eps': 828,\n",
       " 'vampiress': 829,\n",
       " 'ganem': 830,\n",
       " 'involvements': 831,\n",
       " 'cineplex': 832,\n",
       " 'suppression': 833,\n",
       " 'memorable': 834,\n",
       " 'nullified': 835,\n",
       " 'hooray': 836,\n",
       " 'raspberry': 837,\n",
       " 'scattergood': 838,\n",
       " 'congratulations': 839,\n",
       " 'hauer': 840,\n",
       " 'dedications': 841,\n",
       " 'immanent': 842,\n",
       " 'cmdr': 843,\n",
       " 'casing': 844,\n",
       " 'denote': 845,\n",
       " 'shen': 846,\n",
       " 'dalmatian': 847,\n",
       " 'imagesi': 848,\n",
       " 'wrassle': 849,\n",
       " 'outragously': 850,\n",
       " 'strombel': 851,\n",
       " 'herbs': 852,\n",
       " 'slices': 853,\n",
       " 'piles': 854,\n",
       " 'residing': 855,\n",
       " 'schrader': 856,\n",
       " 'alonzo': 857,\n",
       " 'mcewee': 858,\n",
       " 'bonde': 859,\n",
       " 'manga': 860,\n",
       " 'nac': 861,\n",
       " 'attainment': 862,\n",
       " 'distribute': 863,\n",
       " 'acute': 864,\n",
       " 'mpho': 865,\n",
       " 'convoy': 866,\n",
       " 'birthplace': 867,\n",
       " 'barest': 868,\n",
       " 'braincells': 869,\n",
       " 'womman': 870,\n",
       " 'aggravation': 871,\n",
       " 'norment': 872,\n",
       " 'rescued': 873,\n",
       " 'opulently': 874,\n",
       " 'yeasty': 875,\n",
       " 'puffed': 876,\n",
       " 'observably': 877,\n",
       " 'rublev': 878,\n",
       " 'shiph': 879,\n",
       " 'expiate': 880,\n",
       " 'pyaar': 881,\n",
       " 'musseum': 882,\n",
       " 'mcmanus': 883,\n",
       " 'conniption': 884,\n",
       " 'journeying': 885,\n",
       " 'royaly': 886,\n",
       " 'parries': 887,\n",
       " 'escargot': 888,\n",
       " 'grills': 889,\n",
       " 'caroling': 890,\n",
       " 'luminous': 891,\n",
       " 'copyright': 892,\n",
       " 'gojoe': 893,\n",
       " 'perdita': 894,\n",
       " 'hankerchief': 895,\n",
       " 'caution': 896,\n",
       " 'serenity': 897,\n",
       " 'pleas': 898,\n",
       " 'modification': 899,\n",
       " 'breakouts': 900,\n",
       " 'solemnly': 901,\n",
       " 'vacation': 902,\n",
       " 'manoj': 903,\n",
       " 'exactitude': 904,\n",
       " 'coupledom': 905,\n",
       " 'constricted': 906,\n",
       " 'tamale': 907,\n",
       " 'mockable': 908,\n",
       " 'prudery': 909,\n",
       " 'ravine': 910,\n",
       " 'vexing': 911,\n",
       " 'knowns': 912,\n",
       " 'perceptive': 913,\n",
       " 'tanya': 914,\n",
       " 'jyotsna': 915,\n",
       " 'cricket': 916,\n",
       " 'pretzels': 917,\n",
       " 'imperial': 918,\n",
       " 'posy': 919,\n",
       " 'beards': 920,\n",
       " 'mikes': 921,\n",
       " 'mikel': 922,\n",
       " 'asphalt': 923,\n",
       " 'mensonges': 924,\n",
       " 'favortites': 925,\n",
       " 'waterfall': 926,\n",
       " 'commendable': 927,\n",
       " 'debi': 928,\n",
       " 'thrones': 929,\n",
       " 'dorkiest': 930,\n",
       " 'gwenllian': 931,\n",
       " 'salvatores': 932,\n",
       " 'wenn': 933,\n",
       " 'zeon': 934,\n",
       " 'abhimaan': 935,\n",
       " 'accents': 936,\n",
       " 'buna': 937,\n",
       " 'enchants': 938,\n",
       " 'towing': 939,\n",
       " 'newberry': 940,\n",
       " 'crotons': 941,\n",
       " 'pagoda': 942,\n",
       " 'bandini': 943,\n",
       " 'greatfully': 944,\n",
       " 'llbean': 945,\n",
       " 'ameliorative': 946,\n",
       " 'injecting': 947,\n",
       " 'hermann': 948,\n",
       " 'contro': 949,\n",
       " 'coastal': 950,\n",
       " 'hotels': 951,\n",
       " 'dominik': 952,\n",
       " 'demolitions': 953,\n",
       " 'opportune': 954,\n",
       " 'sprites': 955,\n",
       " 'indestructible': 956,\n",
       " 'unmoored': 957,\n",
       " 'saviours': 958,\n",
       " 'contradictory': 959,\n",
       " 'vacuousness': 960,\n",
       " 'pabulum': 961,\n",
       " 'ithought': 962,\n",
       " 'gruen': 963,\n",
       " 'boggles': 964,\n",
       " 'kyoto': 965,\n",
       " 'corpse': 966,\n",
       " 'convulsively': 967,\n",
       " 'moviewise': 968,\n",
       " 'littauer': 969,\n",
       " 'hypocritical': 970,\n",
       " 'panic': 971,\n",
       " 'stinger': 972,\n",
       " 'roflmao': 973,\n",
       " 'kants': 974,\n",
       " 'damini': 975,\n",
       " 'leafy': 976,\n",
       " 'benedek': 977,\n",
       " 'volatile': 978,\n",
       " 'accuracy': 979,\n",
       " 'pickwick': 980,\n",
       " 'couches': 981,\n",
       " 'deplorable': 982,\n",
       " 'kohler': 983,\n",
       " 'pasts': 984,\n",
       " 'engrosses': 985,\n",
       " 'linney': 986,\n",
       " 'slovenians': 987,\n",
       " 'majored': 988,\n",
       " 'doctor': 989,\n",
       " 'chahta': 990,\n",
       " 'amenbar': 991,\n",
       " 'mumu': 992,\n",
       " 'peta': 993,\n",
       " 'hoboken': 994,\n",
       " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz': 995,\n",
       " 'aknowledge': 996,\n",
       " 'sari': 997,\n",
       " 'avowedly': 998,\n",
       " 'iceberg': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word to index\n",
    "word2index = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_input_layer(review):\n",
    "    global layer_0\n",
    "    layer_0 *= 0\n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1\n",
    "\n",
    "update_input_layer(reviews_clean[0])\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_for_label(label):\n",
    "    if(label == \"positive\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "get_target_for_label(labels_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        # set our random number generator \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] += 1\n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'positive'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "\n",
    "            # Hidden layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # TODO: Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # TODO: Update the weights\n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            if(np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews_clean[:-1000], labels_clean[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):797.2% #Correct:500 #Tested:1000 Testing Accuracy:50.0%"
     ]
    }
   ],
   "source": [
    "# evaluate our model before training (just to show how horrible it is)\n",
    "mlp.test(reviews_clean[-1000:],labels_clean[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:5.50% Speed(reviews/sec):154.5 #Correct:661 #Trained:1323 Training Accuracy:49.9%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-22fad3e31860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-791649e0dc3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_reviews, training_labels)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[1;31m# TODO: Update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_1_2\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlayer_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_2_delta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;31m# update hidden-to-output weights with gradient descent step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_0_1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_1_delta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;31m# update input-to-hidden weights with gradient descent step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_2_error\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.train(reviews_clean[:-1000],labels_clean[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews_clean[:-1000],labels_clean[:-1000], learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:2.18% Speed(reviews/sec):153.8 #Correct:260 #Trained:526 Training Accuracy:49.4%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dccd702b61f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-791649e0dc3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_reviews, training_labels)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mreviews_per_second\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\rProgress:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"% Speed(reviews/sec):\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" #Correct:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_so_far\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" #Trained:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Training Accuracy:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_so_far\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send (zmq\\backend\\cython\\socket.c:7305)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send (zmq\\backend\\cython\\socket.c:7048)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy (zmq\\backend\\cython\\socket.c:2920)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq\\backend\\cython\\socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "mlp.train(reviews_clean[:-1000],labels_clean[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews_clean[:-1000],labels_clean[:-1000], learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:10.0% Speed(reviews/sec):151.3 #Correct:1226 #Trained:2423 Training Accuracy:50.5%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-dccd702b61f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-791649e0dc3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_reviews, training_labels)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[1;31m# TODO: Update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_1_2\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlayer_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_2_delta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;31m# update hidden-to-output weights with gradient descent step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_0_1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_1_delta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;31m# update input-to-hidden weights with gradient descent step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_2_error\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "mlp.train(reviews_clean[:-1000],labels_clean[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By reducing the learning rate, the neural network is not improving the accuracy fast enough, so we need to change the structure to improve the model\n",
    "\n",
    "### Analyze the signal vs. noise\n",
    "\n",
    "## Understand Neural Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[0]\n",
    "# A noise here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " '',\n",
       " 'such',\n",
       " 'as',\n",
       " '',\n",
       " 'teachers',\n",
       " '',\n",
       " '.',\n",
       " 'my',\n",
       " '',\n",
       " '',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " '',\n",
       " 's',\n",
       " 'satire',\n",
       " 'is',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'reality',\n",
       " 'than',\n",
       " 'is',\n",
       " '',\n",
       " 'teachers',\n",
       " '',\n",
       " '.',\n",
       " 'the',\n",
       " 'scramble',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'financially',\n",
       " '',\n",
       " 'the',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'who',\n",
       " 'can',\n",
       " 'see',\n",
       " 'right',\n",
       " 'through',\n",
       " 'their',\n",
       " 'pathetic',\n",
       " 'teachers',\n",
       " '',\n",
       " 'pomp',\n",
       " '',\n",
       " 'the',\n",
       " 'pettiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'situation',\n",
       " '',\n",
       " 'all',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'schools',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'and',\n",
       " 'their',\n",
       " 'students',\n",
       " '.',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'which',\n",
       " 'a',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'burn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'school',\n",
       " '',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'at',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'high',\n",
       " '.',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'line',\n",
       " 'inspector',\n",
       " 'i',\n",
       " '',\n",
       " 'm',\n",
       " 'here',\n",
       " 'to',\n",
       " 'sack',\n",
       " 'one',\n",
       " 'of',\n",
       " 'your',\n",
       " 'teachers',\n",
       " '.',\n",
       " 'student',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " '.',\n",
       " 'i',\n",
       " 'expect',\n",
       " 'that',\n",
       " 'many',\n",
       " 'adults',\n",
       " 'of',\n",
       " 'my',\n",
       " 'age',\n",
       " 'think',\n",
       " 'that',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'far',\n",
       " 'fetched',\n",
       " '.',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " 'that',\n",
       " 'it',\n",
       " 'isn',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_clean[0].split(\" \")\n",
    "# A lot of empty, period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 27),\n",
       " ('', 18),\n",
       " ('the', 9),\n",
       " ('to', 6),\n",
       " ('high', 5),\n",
       " ('i', 5),\n",
       " ('bromwell', 4),\n",
       " ('is', 4),\n",
       " ('a', 4),\n",
       " ('teachers', 4),\n",
       " ('that', 4),\n",
       " ('of', 4),\n",
       " ('it', 2),\n",
       " ('at', 2),\n",
       " ('as', 2),\n",
       " ('school', 2),\n",
       " ('my', 2),\n",
       " ('in', 2),\n",
       " ('me', 2),\n",
       " ('students', 2),\n",
       " ('their', 2),\n",
       " ('student', 2),\n",
       " ('cartoon', 1),\n",
       " ('comedy', 1),\n",
       " ('ran', 1),\n",
       " ('same', 1),\n",
       " ('time', 1),\n",
       " ('some', 1),\n",
       " ('other', 1),\n",
       " ('programs', 1),\n",
       " ('about', 1),\n",
       " ('life', 1),\n",
       " ('such', 1),\n",
       " ('years', 1),\n",
       " ('teaching', 1),\n",
       " ('profession', 1),\n",
       " ('lead', 1),\n",
       " ('believe', 1),\n",
       " ('s', 1),\n",
       " ('satire', 1),\n",
       " ('much', 1),\n",
       " ('closer', 1),\n",
       " ('reality', 1),\n",
       " ('than', 1),\n",
       " ('scramble', 1),\n",
       " ('survive', 1),\n",
       " ('financially', 1),\n",
       " ('insightful', 1),\n",
       " ('who', 1),\n",
       " ('can', 1),\n",
       " ('see', 1),\n",
       " ('right', 1),\n",
       " ('through', 1),\n",
       " ('pathetic', 1),\n",
       " ('pomp', 1),\n",
       " ('pettiness', 1),\n",
       " ('whole', 1),\n",
       " ('situation', 1),\n",
       " ('all', 1),\n",
       " ('remind', 1),\n",
       " ('schools', 1),\n",
       " ('knew', 1),\n",
       " ('and', 1),\n",
       " ('when', 1),\n",
       " ('saw', 1),\n",
       " ('episode', 1),\n",
       " ('which', 1),\n",
       " ('repeatedly', 1),\n",
       " ('tried', 1),\n",
       " ('burn', 1),\n",
       " ('down', 1),\n",
       " ('immediately', 1),\n",
       " ('recalled', 1),\n",
       " ('classic', 1),\n",
       " ('line', 1),\n",
       " ('inspector', 1),\n",
       " ('m', 1),\n",
       " ('here', 1),\n",
       " ('sack', 1),\n",
       " ('one', 1),\n",
       " ('your', 1),\n",
       " ('welcome', 1),\n",
       " ('expect', 1),\n",
       " ('many', 1),\n",
       " ('adults', 1),\n",
       " ('age', 1),\n",
       " ('think', 1),\n",
       " ('far', 1),\n",
       " ('fetched', 1),\n",
       " ('what', 1),\n",
       " ('pity', 1),\n",
       " ('isn', 1),\n",
       " ('t', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many period is in the review\n",
    "review_counter = Counter()\n",
    "for word in reviews_clean[0].split(\" \"):\n",
    "    review_counter[word] += 1\n",
    "review_counter.most_common()\n",
    "# The result below shows the dominant word has nothing to do\n",
    "# with the sentiment, the weighting has a dominant effect on the hidden layer\n",
    "# the count weighs heavily on the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Noise in the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork_2:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        # set our random number generator \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "        # While update layers, do not increment, change the counts to binary\n",
    "        # eliminate neural noises\n",
    "        \n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'positive'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "\n",
    "            # Hidden layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # TODO: Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # TODO: Update the weights\n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            if(np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_2 = SentimentNetwork_2(reviews_clean[:-1000], labels_clean[:-1000], learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:10.4% Speed(reviews/sec):155.7 #Correct:1782 #Trained:2501 Training Accuracy:71.2%\n",
      "Progress:20.8% Speed(reviews/sec):156.5 #Correct:3758 #Trained:5001 Training Accuracy:75.1%\n",
      "Progress:31.2% Speed(reviews/sec):157.5 #Correct:5843 #Trained:7501 Training Accuracy:77.8%\n",
      "Progress:41.6% Speed(reviews/sec):156.2 #Correct:7983 #Trained:10001 Training Accuracy:79.8%\n",
      "Progress:52.0% Speed(reviews/sec):153.4 #Correct:10108 #Trained:12501 Training Accuracy:80.8%\n",
      "Progress:62.5% Speed(reviews/sec):153.2 #Correct:12235 #Trained:15001 Training Accuracy:81.5%\n",
      "Progress:72.9% Speed(reviews/sec):152.9 #Correct:14347 #Trained:17501 Training Accuracy:81.9%\n",
      "Progress:83.3% Speed(reviews/sec):152.7 #Correct:16535 #Trained:20001 Training Accuracy:82.6%\n",
      "Progress:93.7% Speed(reviews/sec):152.7 #Correct:18724 #Trained:22501 Training Accuracy:83.2%\n",
      "Progress:99.9% Speed(reviews/sec):152.3 #Correct:20045 #Trained:24000 Training Accuracy:83.5%"
     ]
    }
   ],
   "source": [
    "mlp_2.train(reviews_clean[:-1000], labels_clean[:-1000])\n",
    "#Significant improvement after removing noise\n",
    "#But the training speed seems really slow a.k.a inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1385.% #Correct:849 #Tested:1000 Testing Accuracy:84.9%"
     ]
    }
   ],
   "source": [
    "mlp_2.test(reviews_clean[-1000:], labels_clean[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inefficiency is caused by the large zeros input\n",
    "1. Only care about the non-zero input calculation in the neural network\n",
    "2. 1 multiplication is also wasting time\n",
    "\n",
    "## Increase code efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork_3:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        # set our random number generator \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "        # While update layers, do not increment, change the counts to binary\n",
    "        # eliminate neural noises\n",
    "        \n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'positive'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        \n",
    "        training_reviews = list()\n",
    "        \n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "            # Can completely skip generating input layer\n",
    "            # self.update_input_layer(review)\n",
    "\n",
    "            # Hidden layer\n",
    "            # layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # TODO: Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # TODO: Update the weights\n",
    "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            #self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "            \n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate\n",
    "            \n",
    "            if(np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        start = time.time()\n",
    "        correct = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start + 0.000001)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "        \n",
    "\n",
    "        # Hidden layer\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        \n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_3 = SentimentNetwork_3(reviews_clean[:-1000], labels_clean[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:10.4% Speed(reviews/sec):1188. #Correct:1779 #Trained:2501 Training Accuracy:71.1%\n",
      "Progress:20.8% Speed(reviews/sec):1144. #Correct:3791 #Trained:5001 Training Accuracy:75.8%\n",
      "Progress:31.2% Speed(reviews/sec):1134. #Correct:5889 #Trained:7501 Training Accuracy:78.5%\n",
      "Progress:41.6% Speed(reviews/sec):1140. #Correct:8023 #Trained:10001 Training Accuracy:80.2%\n",
      "Progress:52.0% Speed(reviews/sec):1133. #Correct:10142 #Trained:12501 Training Accuracy:81.1%\n",
      "Progress:62.5% Speed(reviews/sec):1135. #Correct:12278 #Trained:15001 Training Accuracy:81.8%\n",
      "Progress:72.9% Speed(reviews/sec):1130. #Correct:14401 #Trained:17501 Training Accuracy:82.2%\n",
      "Progress:83.3% Speed(reviews/sec):1128. #Correct:16569 #Trained:20001 Training Accuracy:82.8%\n",
      "Progress:93.7% Speed(reviews/sec):1125. #Correct:18753 #Trained:22501 Training Accuracy:83.3%\n",
      "Progress:99.9% Speed(reviews/sec):1126. #Correct:20077 #Trained:24000 Training Accuracy:83.6%"
     ]
    }
   ],
   "source": [
    "mlp_3.train(reviews_clean[:-1000], labels_clean[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1740.% #Correct:500 #Tested:1000 Testing Accuracy:50.0%"
     ]
    }
   ],
   "source": [
    "mlp_3.test(reviews_clean[-1000:], labels_clean[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork_3T:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self,reviews):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            self.layer_0[0][self.word2index[word]] = 1\n",
    "\n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'positive'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        \n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "\n",
    "            # Hidden layer\n",
    "#             layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # Update the weights\n",
    "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            \n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            if(np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "        \n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start + 0.0000001)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "\n",
    "\n",
    "        # Hidden layer\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_3_T = SentimentNetwork_3T(reviews_clean[:-1000], labels_clean[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1124. #Correct:20077 #Trained:24000 Training Accuracy:83.6%"
     ]
    }
   ],
   "source": [
    "mlp_3_T.train(reviews_clean[:-1000], labels_clean[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1141. #Correct:41747 #Trained:48000 Training Accuracy:86.9%"
     ]
    }
   ],
   "source": [
    "mlp_3_T.train(reviews_clean[:-1000] * 2, labels_clean[:-1000] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1839.% #Correct:853 #Tested:1000 Testing Accuracy:85.3%"
     ]
    }
   ],
   "source": [
    "mlp_3_T.test(reviews_clean[-1000:], labels_clean[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
