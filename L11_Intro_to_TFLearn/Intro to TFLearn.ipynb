{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro to TFLearn\n",
    "TFLearn is used to build neural networks for sentiment analysis.  \n",
    "The library does a lot of the dirty works such as *initializing weights, running the forward pass, and taking care of the backpropagation* (Looks like **All** the work of building a neural network to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Activation Functions\n",
    "In the past, **sigmoid function** is the main function used as the activation function. This is not the only activation function used and it has some drawbacks:\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5893d15c_sigmoids/sigmoids.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- Error shrinking: The derivative of sigmoid funcion maxes out at 0.25, meaning when performing backpropagation with sigmoid, the errors going back into the network will be shrunk by at least 75% at every layer. For models with a lot of layers the weight updates will be tiny.\n",
    "As a result, sigmoids should not be chosen as activations on hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Rectified Linear Units (ReLUs)\n",
    "\n",
    "### ReLu Definition\n",
    "Most recent deep learning networks use **rectified linear units (ReLUs)** for the hidden layers.  \n",
    "Mathematically:  \n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tx  & \\mbox{if } x > 0 \\\\\n",
    "\t\t0 & \\mbox{if } x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Graphically, it looks like:  \n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58915ae8_relu/relu.png\" alt=\"Drawing\" style=\"height: 300px;\"/>\n",
    "\n",
    "### Drawbacks\n",
    "It's possible that a large gradient can set the weights such that a ReLU unit will always be 0. These \"dead\" units will always be 0 and a lot of computation will be wasted in training.\n",
    "\n",
    "From Andrej Karpath:\n",
    ">Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax\n",
    "The softmax function **squashes the outputs of each unit** to be between 0 and 1, just like a sigmoid. It also divides each output such that **the total sum of the outputs is equal to 1**. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.  \n",
    "The only real difference between softmax and sigmoid is that the softmax normalizes the outputs so that they sum to 1.  \n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png\" alt=\"Drawing\" style=\"height: 100px;\"/>  \n",
    "Mathematically:  \n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58938e9e_softmax-math/softmax-math.png\" alt=\"Drawing\" style=\"height: 50px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Categorical Cross-Entropy\n",
    "For previous gradient descent the sum of squared errors were used as the cost function in the networks, but in those cases we only have singular (scalar) output values.\n",
    "When using **softmax** the output is a vector.  \n",
    "Can also express your data labels as a vector using what's called **one-hot encoding**.  \n",
    "Cross entropy calculates *how far apart label vector vs. predicted vector*\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589b18f5_cross-entropy-diagram/cross-entropy-diagram.png\" alt=\"Drawing\" style=\"height: 150px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sentiment Analysis with TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
