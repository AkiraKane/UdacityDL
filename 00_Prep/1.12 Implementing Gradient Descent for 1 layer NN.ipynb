{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "- Use gradient descent to train a network on graduate school admissions data\n",
    "- File path \"/home/isaac/UdacityDL/00_Prep/binary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "- we actually need to transform the data first. The rank feature is categorical, the numbers don't encode any sort of relative values. Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2. Instead, we need to use dummy variables to encode rank, splitting the data into four new columns encoded with ones or zeros.\n",
    "- We'll also need to standardize the GRE and GPA data, which means to scale the values such they have zero mean and a standard deviation of 1. This is necessary because the sigmoid function squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Explained\n",
    "- Set the weight step to zero: $\\Delta w_{i} = 0$\n",
    "- For each record in the training data\n",
    "    1. Make a forward pass through the network, calculating the output $\\hat y = f(\\sum_{i}w_{i}x_{i})$\n",
    "    2. Calculate the error gradient in the output unit, $\\delta = (y - \\hat y) \\times f'(\\sum_{i}w_{i}x_{i})$\n",
    "    3. Update the weight step $\\Delta w_{i} = w_{i} + \\delta x_{i}$\n",
    "- Update the weights $ \\large w_{i} = w_{i} + \\frac {\\eta \\Delta w_{i}} {m}, \\hspace{2mm}$ where $\\eta$ is the learning rate and m is the number of records\n",
    "- Repeat for $e$ epochs\n",
    "\n",
    "#### Notes:\n",
    "- Use Sigmoid for the activation function: $ \\large f(h) = \\frac {1}{1 + e^{-x}}$\n",
    "- Gradient of the Sigmoid is $f'(h) = f(h) (1 - f(h))$\n",
    "- $h$ is the input to the output unit: $h = \\sum_{i}w_{i}x_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Need to initialize the weights\n",
    "2. Weights should be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends\n",
    "3. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry\n",
    "4. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is $\\large \\frac{1}{\\sqrt n}$ where $n$ is the number of input units. This keeps input to the sigmoid low for increasing numbers of input units\n",
    "\n",
    "Initialize weights\n",
    "weights = np.random.normal(scale = 1/n_features ** (-0.5), size = n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Steps for Gradient Descent\n",
    "def run():\n",
    "    #set seed to generate same results\n",
    "    np.random.seed(42)\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "    \n",
    "    # Initialize weights\n",
    "    weights = np.random.normal(scale = 1/n_features ** (-0.5), size = n_features)\n",
    "    \n",
    "    # Set up NN hyperparameters\n",
    "    epochs = 1000\n",
    "    learnrate = 0.5\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(len(weights))\n",
    "        for x,y in zip(features.values, targets):\n",
    "            ## 1. Calculate the output\n",
    "            nn_output = sigmoid(np.dot(weights, x))\n",
    "            ## 2. Calculate the error & Gradient Error\n",
    "            nn_error = y - nn_output\n",
    "            nn_grd_error = nn_error * nn_output * (1 - nn_output)\n",
    "            ## 3. Calculate the weight change\n",
    "            del_w += nn_grd_error * x\n",
    "        weights += learnrate * del_w / n_records\n",
    "        \n",
    "        #Printing the MSE on the training set\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean((out - targets) ** 2)\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Training Loss: \", loss, \"Warning - loss increasing\")\n",
    "            else:\n",
    "                print(\"Training Loss: \", loss)\n",
    "            last_loss = loss\n",
    "    \n",
    "    #Calculate accuracy on test data\n",
    "    test_out = sigmoid(np.dot(features_test, weights))\n",
    "    prediction = test_out > 0.5\n",
    "    accuracy = np.mean(prediction == targets_test)\n",
    "    print(\"Prediction Accuracy: {a}\".format(a = accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.368439541983512\n",
      "Training Loss:  0.349170785218876\n",
      "Training Loss:  0.33217440574485835\n",
      "Training Loss:  0.30603721927356853\n",
      "Training Loss:  0.25612134085589033\n",
      "Training Loss:  0.21026563561120573\n",
      "Training Loss:  0.19921050410028213\n",
      "Training Loss:  0.1974844717751469\n",
      "Training Loss:  0.1971205928346378\n",
      "Training Loss:  0.19702125482094843\n",
      "Prediction Accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "#Write the main function\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
